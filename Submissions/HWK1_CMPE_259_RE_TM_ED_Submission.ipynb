{"cells":[{"cell_type":"markdown","source":["# CMPE 297: Homework 1: Regular expressions, text normalization, and edit distance\n","\n","The parts that you need to complete are marked as Exercises."],"metadata":{"id":"eGajpTvZbl8W"},"id":"eGajpTvZbl8W"},{"cell_type":"markdown","source":["## Part 0: Initialization & Setup"],"metadata":{"id":"lkzAo9kSbpaa"},"id":"lkzAo9kSbpaa"},{"cell_type":"code","execution_count":null,"id":"d56c92a0","metadata":{"id":"d56c92a0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693549841708,"user_tz":420,"elapsed":4524,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"b012813a-ea5f-4697-eab9-07f08463926c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/movie_reviews.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["# importing required libraries\n","import re\n","import nltk\n","from nltk.corpus import movie_reviews\n","import string\n","import pandas as pd\n","from nltk.corpus import stopwords\n","\n","nltk.download('movie_reviews')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"markdown","source":["\n","\n","## Part 1: Regular Expressions"],"metadata":{"id":"2jEV5pB3bhkq"},"id":"2jEV5pB3bhkq"},{"cell_type":"markdown","source":["### Extracting license plate numbers, IDs, emails and mailing addresses from a document\n"],"metadata":{"id":"oLP8HLkcbvcG"},"id":"oLP8HLkcbvcG"},{"cell_type":"markdown","source":["#### Document creation"],"metadata":{"id":"xLT-yj6keZOF"},"id":"xLT-yj6keZOF"},{"cell_type":"code","execution_count":null,"id":"aefa7f39","metadata":{"id":"aefa7f39","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1693549849250,"user_tz":420,"elapsed":298,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"dc0525b2-6cb2-4ad4-f512-0fd7bb3fdad0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'I am 20 years old. My previous license plate number was 4XUI302 and my new one is 3A-278. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["sentence = 'I am 20 years old. My previous license plate number was 4XUI302 and my new one is 3A-278. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'\n","sentence"]},{"cell_type":"markdown","source":["Extracting license plate numbers"],"metadata":{"id":"enF7P05qebea"},"id":"enF7P05qebea"},{"cell_type":"code","execution_count":null,"id":"65e2f32e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65e2f32e","outputId":"96da6312-3f59-40a3-9bff-eca0becd6aa2","executionInfo":{"status":"ok","timestamp":1693549852250,"user_tz":420,"elapsed":12,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['4XUI302', '3A-278']"]},"metadata":{},"execution_count":4}],"source":["# The format of license plate number is a digit then 2 or 3 letters (one of which can be a \"-\"), and then 3 digits\n","\n","regex = re.compile(r'(\\d{1}[A-Za-z-]{2,3}\\d{3})')\n","lincense_plate_numbers = regex.findall(sentence)\n","lincense_plate_numbers"]},{"cell_type":"markdown","source":["### Exercise 1-1: Extract the ID numbers from the document."],"metadata":{"id":"nzg5Gxx9dzW2"},"id":"nzg5Gxx9dzW2"},{"cell_type":"code","execution_count":null,"id":"d98e769f","metadata":{"id":"d98e769f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693549853551,"user_tz":420,"elapsed":4,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"d6b70cfa-1139-4df6-e6a8-4566cee5c4be"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['J987492']"]},"metadata":{},"execution_count":5}],"source":["# The format of the IDs is one character/letter and then 6 digits\n","regex = re.compile(r'(\\w{1}\\d{6})')\n","ids = regex.findall(sentence)\n","ids"]},{"cell_type":"markdown","source":["### Exercise 1-2: Extract the email IDs from the document"],"metadata":{"id":"y3BZc47FeRzR"},"id":"y3BZc47FeRzR"},{"cell_type":"code","execution_count":null,"id":"11287af4","metadata":{"id":"11287af4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693549853896,"user_tz":420,"elapsed":5,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"dc6a9639-8d0d-499a-b768-45a1eec4bba8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['myemail123+spam@google.cg', 'jane.doe@sjsu.edu']"]},"metadata":{},"execution_count":6}],"source":["regex = re.compile(r'([\\d\\w+.]*[@]+[\\w\\d]+[.]+[a-z]*)')\n","emails = regex.findall(sentence)\n","emails"]},{"cell_type":"markdown","source":["### Exercise 1-3: Extract the mailing address from the document"],"metadata":{"id":"UfCxo2u2erDf"},"id":"UfCxo2u2erDf"},{"cell_type":"code","execution_count":null,"id":"62515169","metadata":{"id":"62515169","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693549855042,"user_tz":420,"elapsed":6,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"23880400-7695-49d9-89ef-9f8a61c83c81"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['123 Main street, San Jose, CA']"]},"metadata":{},"execution_count":7}],"source":["regex = re.compile(r'([0-9 ]{3}[A-Za-z ]*[,]{1}[A-Za-z0-9 ]*[,]{1}[\\w ]*)')\n","mailing_address = regex.findall(sentence)\n","mailing_address"]},{"cell_type":"markdown","source":["### Exercise 1-4: Anonymize the license plate numbers by replacing them with the text \"LP_NUM\"\n","\n","The re.sub function is described here: https://docs.python.org/3/library/re.html"],"metadata":{"id":"uM53UdvPevrA"},"id":"uM53UdvPevrA"},{"cell_type":"code","execution_count":null,"id":"ca6ae08c","metadata":{"id":"ca6ae08c","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1693549856237,"user_tz":420,"elapsed":8,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"44414009-3d34-420e-d6df-c643f49f7a80"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'I am 20 years old. My previous license plate number was LP_NUM and my new one is LP_NUM. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["# Now replacing license plate numbers with the string \"LP_NUM\"\n","sentence_modified = re.sub(r'(\\d{1}[A-Za-z-]{2,3}\\d{3})',r'LP_NUM',sentence)\n","sentence_modified"]},{"cell_type":"markdown","source":["### Exercise 1-5: Replace the ID numbers with the text \"ID_NUM\""],"metadata":{"id":"rArCsPyMfAeZ"},"id":"rArCsPyMfAeZ"},{"cell_type":"code","source":["sentence_modified = re.sub(r'(\\w{1}\\d{6})',r'ID_NUM',sentence)\n","sentence_modified"],"metadata":{"id":"glVWmQAOfFTU","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1693549857426,"user_tz":420,"elapsed":7,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"723981ab-a111-4ece-fc11-80745a20c0e9"},"id":"glVWmQAOfFTU","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'I am 20 years old. My previous license plate number was 4XUI302 and my new one is 3A-278. My ID is ID_NUM and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["## Part 2: Text Processing"],"metadata":{"id":"q2ymEa7sfHnL"},"id":"q2ymEa7sfHnL"},{"cell_type":"markdown","source":["Count the number of words in the movie_reviews dataset (dataset uploaded in the beginning of this notebook under \"Part 0: Initialization and Setup\")"],"metadata":{"id":"F_Y77JT9fjid"},"id":"F_Y77JT9fjid"},{"cell_type":"code","execution_count":null,"id":"3239e17c","metadata":{"id":"3239e17c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693549862882,"user_tz":420,"elapsed":2962,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"cb042020-b15b-4467-ab43-67c1e4ba5c9b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1583820"]},"metadata":{},"execution_count":10}],"source":["# print number of words in the movie review dataset\n","len(movie_reviews.words())"]},{"cell_type":"markdown","source":["Load the standard list of punctuation marks"],"metadata":{"id":"86nbNEYwfgwl"},"id":"86nbNEYwfgwl"},{"cell_type":"code","execution_count":null,"id":"0929c795","metadata":{"id":"0929c795","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1693549863903,"user_tz":420,"elapsed":5,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"465ef41b-df84-48d7-ec07-ac88a3367513"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}],"source":["punctuations = string.punctuation\n","punctuations"]},{"cell_type":"markdown","source":["Remove punctation from movie reviews\n"],"metadata":{"id":"sg5Sc7X9fbpg"},"id":"sg5Sc7X9fbpg"},{"cell_type":"code","execution_count":null,"id":"18760b2a","metadata":{"id":"18760b2a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693549868410,"user_tz":420,"elapsed":1779,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"d0c0b9e9-fec7-4c77-e3f1-994cb00a5ba6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1338788"]},"metadata":{},"execution_count":12}],"source":["words_wo_puncts = [x for x in movie_reviews.words() if x not in punctuations]\n","len(words_wo_puncts)"]},{"cell_type":"markdown","source":["Count the number of unique words"],"metadata":{"id":"Gvl4C9l8f0M-"},"id":"Gvl4C9l8f0M-"},{"cell_type":"code","execution_count":null,"id":"0a18f452","metadata":{"id":"0a18f452","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693549870935,"user_tz":420,"elapsed":564,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"4b7e0c0d-db63-4ac0-bb86-c5117ab24d2d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["39737"]},"metadata":{},"execution_count":13}],"source":["unique_words = set(words_wo_puncts)\n","len(unique_words)"]},{"cell_type":"markdown","source":["Find the 20 most frequent words in the dataset"],"metadata":{"id":"D2L7yKz3gL-h"},"id":"D2L7yKz3gL-h"},{"cell_type":"code","execution_count":null,"id":"784d9a68","metadata":{"id":"784d9a68","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693549874730,"user_tz":420,"elapsed":565,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"43609656-8827-4c4c-a84a-967d0c791d19"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["the     76529\n","a       38106\n","and     35576\n","of      34123\n","to      31937\n","is      25195\n","in      21822\n","s       18513\n","it      16107\n","that    15924\n","as      11378\n","with    10792\n","for      9961\n","his      9587\n","this     9578\n","film     9517\n","i        8889\n","he       8864\n","but      8634\n","on       7385\n","dtype: int64"]},"metadata":{},"execution_count":14}],"source":["# top 20 highest freq words\n","pd.Series(words_wo_puncts).value_counts()[:20]"]},{"cell_type":"markdown","source":["Load the standard list of stopwords"],"metadata":{"id":"id9YqucXf6oW"},"id":"id9YqucXf6oW"},{"cell_type":"code","execution_count":null,"id":"9de2e57e","metadata":{"id":"9de2e57e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693549879763,"user_tz":420,"elapsed":233,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"b3e14877-9996-4af0-d9c7-c02dc0e93e15"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i',\n"," 'me',\n"," 'my',\n"," 'myself',\n"," 'we',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'you',\n"," \"you're\",\n"," \"you've\",\n"," \"you'll\",\n"," \"you'd\",\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves',\n"," 'he',\n"," 'him',\n"," 'his',\n"," 'himself',\n"," 'she',\n"," \"she's\",\n"," 'her',\n"," 'hers',\n"," 'herself',\n"," 'it',\n"," \"it's\",\n"," 'its',\n"," 'itself',\n"," 'they',\n"," 'them',\n"," 'their',\n"," 'theirs',\n"," 'themselves',\n"," 'what',\n"," 'which',\n"," 'who',\n"," 'whom',\n"," 'this',\n"," 'that',\n"," \"that'll\",\n"," 'these',\n"," 'those',\n"," 'am',\n"," 'is',\n"," 'are',\n"," 'was',\n"," 'were',\n"," 'be',\n"," 'been',\n"," 'being',\n"," 'have',\n"," 'has',\n"," 'had',\n"," 'having',\n"," 'do',\n"," 'does',\n"," 'did',\n"," 'doing',\n"," 'a',\n"," 'an',\n"," 'the',\n"," 'and',\n"," 'but',\n"," 'if',\n"," 'or',\n"," 'because',\n"," 'as',\n"," 'until',\n"," 'while',\n"," 'of',\n"," 'at',\n"," 'by',\n"," 'for',\n"," 'with',\n"," 'about',\n"," 'against',\n"," 'between',\n"," 'into',\n"," 'through',\n"," 'during',\n"," 'before',\n"," 'after',\n"," 'above',\n"," 'below',\n"," 'to',\n"," 'from',\n"," 'up',\n"," 'down',\n"," 'in',\n"," 'out',\n"," 'on',\n"," 'off',\n"," 'over',\n"," 'under',\n"," 'again',\n"," 'further',\n"," 'then',\n"," 'once',\n"," 'here',\n"," 'there',\n"," 'when',\n"," 'where',\n"," 'why',\n"," 'how',\n"," 'all',\n"," 'any',\n"," 'both',\n"," 'each',\n"," 'few',\n"," 'more',\n"," 'most',\n"," 'other',\n"," 'some',\n"," 'such',\n"," 'no',\n"," 'nor',\n"," 'not',\n"," 'only',\n"," 'own',\n"," 'same',\n"," 'so',\n"," 'than',\n"," 'too',\n"," 'very',\n"," 's',\n"," 't',\n"," 'can',\n"," 'will',\n"," 'just',\n"," 'don',\n"," \"don't\",\n"," 'should',\n"," \"should've\",\n"," 'now',\n"," 'd',\n"," 'll',\n"," 'm',\n"," 'o',\n"," 're',\n"," 've',\n"," 'y',\n"," 'ain',\n"," 'aren',\n"," \"aren't\",\n"," 'couldn',\n"," \"couldn't\",\n"," 'didn',\n"," \"didn't\",\n"," 'doesn',\n"," \"doesn't\",\n"," 'hadn',\n"," \"hadn't\",\n"," 'hasn',\n"," \"hasn't\",\n"," 'haven',\n"," \"haven't\",\n"," 'isn',\n"," \"isn't\",\n"," 'ma',\n"," 'mightn',\n"," \"mightn't\",\n"," 'mustn',\n"," \"mustn't\",\n"," 'needn',\n"," \"needn't\",\n"," 'shan',\n"," \"shan't\",\n"," 'shouldn',\n"," \"shouldn't\",\n"," 'wasn',\n"," \"wasn't\",\n"," 'weren',\n"," \"weren't\",\n"," 'won',\n"," \"won't\",\n"," 'wouldn',\n"," \"wouldn't\"]"]},"metadata":{},"execution_count":15}],"source":["# getting english stopwords\n","eng_stopwords = stopwords.words('english')\n","eng_stopwords"]},{"cell_type":"markdown","source":["Count the number of stopwords"],"metadata":{"id":"cL-RrEeCgA0Z"},"id":"cL-RrEeCgA0Z"},{"cell_type":"code","execution_count":null,"id":"d6645fe0","metadata":{"id":"d6645fe0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693549882662,"user_tz":420,"elapsed":234,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"a49ed0dc-8d61-4f13-e9b6-7f890cbfeb20"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["179"]},"metadata":{},"execution_count":16}],"source":["len(eng_stopwords)"]},{"cell_type":"markdown","source":["### Exercise 2-1: Remove the stopwords from the dataset (similarly to how we removed punctuation above)"],"metadata":{"id":"xBAO2VeWgDiM"},"id":"xBAO2VeWgDiM"},{"cell_type":"code","execution_count":null,"id":"9d584a6f","metadata":{"id":"9d584a6f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693549893645,"user_tz":420,"elapsed":2795,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"fe231a8c-0c6f-44be-bf08-213c9966773d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["710578"]},"metadata":{},"execution_count":17}],"source":["words_wo_puncts_stopwords = [x for x in words_wo_puncts if (x not in eng_stopwords)]\n","len(words_wo_puncts_stopwords)"]},{"cell_type":"markdown","source":["### Exercise 2-2: Find the number of uniques words in the dataset now that the stop words have been removed"],"metadata":{"id":"RuUty50kgS2o"},"id":"RuUty50kgS2o"},{"cell_type":"code","execution_count":null,"id":"a92f5eb5","metadata":{"id":"a92f5eb5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693549896302,"user_tz":420,"elapsed":363,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"33907253-4203-445b-9cc8-a89c47206959"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["39586"]},"metadata":{},"execution_count":18}],"source":["# unique words without stopwords\n","unique_words = set(words_wo_puncts_stopwords)\n","len(unique_words)"]},{"cell_type":"markdown","source":["### Exercise 2-3: Find the top 20 highest frequency words now that we have removed the stopwords"],"metadata":{"id":"bxDxQWNegcny"},"id":"bxDxQWNegcny"},{"cell_type":"code","execution_count":null,"id":"840dded3","metadata":{"id":"840dded3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693549898896,"user_tz":420,"elapsed":231,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"b29fbab8-af19-4ab3-9d81-09f0f53aac85"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["film          9517\n","one           5852\n","movie         5771\n","like          3690\n","even          2565\n","time          2411\n","good          2411\n","story         2169\n","would         2109\n","much          2049\n","character     2020\n","also          1967\n","get           1949\n","two           1911\n","well          1906\n","characters    1859\n","first         1836\n","--            1815\n","see           1749\n","way           1693\n","dtype: int64"]},"metadata":{},"execution_count":19}],"source":["# top 20 highest freq words after removing stopwords\n","pd.Series(words_wo_puncts_stopwords).value_counts()[:20]\n","# pd.Series(words_wo_puncts).value_counts()[:20]\n","\n"]},{"cell_type":"markdown","source":["Find the words that are used only once in the corpus (and print the first few).  "],"metadata":{"id":"9Z36G7BcgmDF"},"id":"9Z36G7BcgmDF"},{"cell_type":"code","execution_count":null,"id":"70748ea0","metadata":{"id":"70748ea0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693549902031,"user_tz":420,"elapsed":1268,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"acd2b282-882a-431f-c689-2926f886095f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['looooot',\n"," 'schnazzy',\n"," 'timex',\n"," 'indiglo',\n"," 'jessalyn',\n"," 'gilsig',\n"," 'ruber',\n"," 'jaleel',\n"," 'balki',\n"," 'wavers',\n"," 'statistics',\n"," 'snapshot',\n"," 'guesswork',\n"," 'maryam',\n"," 'daylights',\n"," 'terraformed',\n"," 'stagnated',\n"," 'napolean',\n"," 'millimeter',\n"," 'enmeshed']"]},"metadata":{},"execution_count":20}],"source":["# 20 words that are used only once in corpus using hapaxes() function\n","nltk.FreqDist(words_wo_puncts_stopwords).hapaxes()[:20]"]},{"cell_type":"markdown","source":["### Exercise 2-4: Use the PorterStemmer to stem the words in the dataset.\n","\n","Display the first few words."],"metadata":{"id":"US3mRSQ8bDei"},"id":"US3mRSQ8bDei"},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","ps = PorterStemmer()\n","words = words_wo_puncts_stopwords[1:200]\n","stemmedwords = []\n","\n","for w in words:\n","  print(w,\"---\",ps.stem(w))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Quw9BeUWhhfV","executionInfo":{"status":"ok","timestamp":1693551321262,"user_tz":420,"elapsed":333,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"6778ecb3-4bef-4207-f95d-3db98d846110"},"id":"Quw9BeUWhhfV","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["two --- two\n","teen --- teen\n","couples --- coupl\n","go --- go\n","church --- church\n","party --- parti\n","drink --- drink\n","drive --- drive\n","get --- get\n","accident --- accid\n","one --- one\n","guys --- guy\n","dies --- die\n","girlfriend --- girlfriend\n","continues --- continu\n","see --- see\n","life --- life\n","nightmares --- nightmar\n","deal --- deal\n","watch --- watch\n","movie --- movi\n","sorta --- sorta\n","find --- find\n","critique --- critiqu\n","mind --- mind\n","fuck --- fuck\n","movie --- movi\n","teen --- teen\n","generation --- gener\n","touches --- touch\n","cool --- cool\n","idea --- idea\n","presents --- present\n","bad --- bad\n","package --- packag\n","makes --- make\n","review --- review\n","even --- even\n","harder --- harder\n","one --- one\n","write --- write\n","since --- sinc\n","generally --- gener\n","applaud --- applaud\n","films --- film\n","attempt --- attempt\n","break --- break\n","mold --- mold\n","mess --- mess\n","head --- head\n","lost --- lost\n","highway --- highway\n","memento --- memento\n","good --- good\n","bad --- bad\n","ways --- way\n","making --- make\n","types --- type\n","films --- film\n","folks --- folk\n","snag --- snag\n","one --- one\n","correctly --- correctli\n","seem --- seem\n","taken --- taken\n","pretty --- pretti\n","neat --- neat\n","concept --- concept\n","executed --- execut\n","terribly --- terribl\n","problems --- problem\n","movie --- movi\n","well --- well\n","main --- main\n","problem --- problem\n","simply --- simpli\n","jumbled --- jumbl\n","starts --- start\n","normal --- normal\n","downshifts --- downshift\n","fantasy --- fantasi\n","world --- world\n","audience --- audienc\n","member --- member\n","idea --- idea\n","going --- go\n","dreams --- dream\n","characters --- charact\n","coming --- come\n","back --- back\n","dead --- dead\n","others --- other\n","look --- look\n","like --- like\n","dead --- dead\n","strange --- strang\n","apparitions --- apparit\n","disappearances --- disappear\n","looooot --- looooot\n","chase --- chase\n","scenes --- scene\n","tons --- ton\n","weird --- weird\n","things --- thing\n","happen --- happen\n","simply --- simpli\n","explained --- explain\n","personally --- person\n","mind --- mind\n","trying --- tri\n","unravel --- unravel\n","film --- film\n","every --- everi\n","give --- give\n","clue --- clue\n","get --- get\n","kind --- kind\n","fed --- fed\n","film --- film\n","biggest --- biggest\n","problem --- problem\n","obviously --- obvious\n","got --- got\n","big --- big\n","secret --- secret\n","hide --- hide\n","seems --- seem\n","want --- want\n","hide --- hide\n","completely --- complet\n","final --- final\n","five --- five\n","minutes --- minut\n","make --- make\n","things --- thing\n","entertaining --- entertain\n","thrilling --- thrill\n","even --- even\n","engaging --- engag\n","meantime --- meantim\n","really --- realli\n","sad --- sad\n","part --- part\n","arrow --- arrow\n","dig --- dig\n","flicks --- flick\n","like --- like\n","actually --- actual\n","figured --- figur\n","half --- half\n","way --- way\n","point --- point\n","strangeness --- strang\n","start --- start\n","make --- make\n","little --- littl\n","bit --- bit\n","sense --- sens\n","still --- still\n","make --- make\n","film --- film\n","entertaining --- entertain\n","guess --- guess\n","bottom --- bottom\n","line --- line\n","movies --- movi\n","like --- like\n","always --- alway\n","make --- make\n","sure --- sure\n","audience --- audienc\n","even --- even\n","given --- given\n","secret --- secret\n","password --- password\n","enter --- enter\n","world --- world\n","understanding --- understand\n","mean --- mean\n","showing --- show\n","melissa --- melissa\n","sagemiller --- sagemil\n","running --- run\n","away --- away\n","visions --- vision\n","20 --- 20\n","minutes --- minut\n","throughout --- throughout\n","movie --- movi\n","plain --- plain\n","lazy --- lazi\n","okay --- okay\n","get --- get\n","people --- peopl\n","chasing --- chase\n","know --- know\n","really --- realli\n","need --- need\n","see --- see\n"]}]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","ps = PorterStemmer()\n","words = words_wo_puncts_stopwords\n","stemmedwords = []\n","\n","for w in words:\n","  # print(w,\"---\",ps.stem(w))\n","#   dis = ps.stem(w)\n","  stemmedwords.append(ps.stem(w))\n","\n","diststemwords = set(stemmedwords)\n","len(diststemwords)"],"metadata":{"id":"nX3r9FfubKdB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693551100239,"user_tz":420,"elapsed":16046,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"a51ecb29-91b0-4f8d-f1b4-60660700c372"},"id":"nX3r9FfubKdB","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["26101"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["### Exercise 2-5: Use the\n","\n","---\n","\n","WordNetLemmatizer to lemmatize the words in the dataset.\n","\n","Display the first few words."],"metadata":{"id":"QEVGhVGTbUMT"},"id":"QEVGhVGTbUMT"},{"cell_type":"code","source":["from nltk import WordNetLemmatizer\n","\n","wl = WordNetLemmatizer()\n","wl_dataset = words_wo_puncts_stopwords[1:200]\n","wl_dataset\n","\n","distinct_wl = []\n","\n","for w in wl_dataset:\n","  print(w,\"-->\",wl.lemmatize(w))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pp8ahNo2hpj0","executionInfo":{"status":"ok","timestamp":1693551353182,"user_tz":420,"elapsed":555,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"2b3d7a99-503d-43b0-8421-a0514693562a"},"id":"pp8ahNo2hpj0","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["two --> two\n","teen --> teen\n","couples --> couple\n","go --> go\n","church --> church\n","party --> party\n","drink --> drink\n","drive --> drive\n","get --> get\n","accident --> accident\n","one --> one\n","guys --> guy\n","dies --> dy\n","girlfriend --> girlfriend\n","continues --> continues\n","see --> see\n","life --> life\n","nightmares --> nightmare\n","deal --> deal\n","watch --> watch\n","movie --> movie\n","sorta --> sorta\n","find --> find\n","critique --> critique\n","mind --> mind\n","fuck --> fuck\n","movie --> movie\n","teen --> teen\n","generation --> generation\n","touches --> touch\n","cool --> cool\n","idea --> idea\n","presents --> present\n","bad --> bad\n","package --> package\n","makes --> make\n","review --> review\n","even --> even\n","harder --> harder\n","one --> one\n","write --> write\n","since --> since\n","generally --> generally\n","applaud --> applaud\n","films --> film\n","attempt --> attempt\n","break --> break\n","mold --> mold\n","mess --> mess\n","head --> head\n","lost --> lost\n","highway --> highway\n","memento --> memento\n","good --> good\n","bad --> bad\n","ways --> way\n","making --> making\n","types --> type\n","films --> film\n","folks --> folk\n","snag --> snag\n","one --> one\n","correctly --> correctly\n","seem --> seem\n","taken --> taken\n","pretty --> pretty\n","neat --> neat\n","concept --> concept\n","executed --> executed\n","terribly --> terribly\n","problems --> problem\n","movie --> movie\n","well --> well\n","main --> main\n","problem --> problem\n","simply --> simply\n","jumbled --> jumbled\n","starts --> start\n","normal --> normal\n","downshifts --> downshift\n","fantasy --> fantasy\n","world --> world\n","audience --> audience\n","member --> member\n","idea --> idea\n","going --> going\n","dreams --> dream\n","characters --> character\n","coming --> coming\n","back --> back\n","dead --> dead\n","others --> others\n","look --> look\n","like --> like\n","dead --> dead\n","strange --> strange\n","apparitions --> apparition\n","disappearances --> disappearance\n","looooot --> looooot\n","chase --> chase\n","scenes --> scene\n","tons --> ton\n","weird --> weird\n","things --> thing\n","happen --> happen\n","simply --> simply\n","explained --> explained\n","personally --> personally\n","mind --> mind\n","trying --> trying\n","unravel --> unravel\n","film --> film\n","every --> every\n","give --> give\n","clue --> clue\n","get --> get\n","kind --> kind\n","fed --> fed\n","film --> film\n","biggest --> biggest\n","problem --> problem\n","obviously --> obviously\n","got --> got\n","big --> big\n","secret --> secret\n","hide --> hide\n","seems --> seems\n","want --> want\n","hide --> hide\n","completely --> completely\n","final --> final\n","five --> five\n","minutes --> minute\n","make --> make\n","things --> thing\n","entertaining --> entertaining\n","thrilling --> thrilling\n","even --> even\n","engaging --> engaging\n","meantime --> meantime\n","really --> really\n","sad --> sad\n","part --> part\n","arrow --> arrow\n","dig --> dig\n","flicks --> flick\n","like --> like\n","actually --> actually\n","figured --> figured\n","half --> half\n","way --> way\n","point --> point\n","strangeness --> strangeness\n","start --> start\n","make --> make\n","little --> little\n","bit --> bit\n","sense --> sense\n","still --> still\n","make --> make\n","film --> film\n","entertaining --> entertaining\n","guess --> guess\n","bottom --> bottom\n","line --> line\n","movies --> movie\n","like --> like\n","always --> always\n","make --> make\n","sure --> sure\n","audience --> audience\n","even --> even\n","given --> given\n","secret --> secret\n","password --> password\n","enter --> enter\n","world --> world\n","understanding --> understanding\n","mean --> mean\n","showing --> showing\n","melissa --> melissa\n","sagemiller --> sagemiller\n","running --> running\n","away --> away\n","visions --> vision\n","20 --> 20\n","minutes --> minute\n","throughout --> throughout\n","movie --> movie\n","plain --> plain\n","lazy --> lazy\n","okay --> okay\n","get --> get\n","people --> people\n","chasing --> chasing\n","know --> know\n","really --> really\n","need --> need\n","see --> see\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"06cWHa3_2Wxw"},"id":"06cWHa3_2Wxw"},{"cell_type":"code","source":["from nltk import WordNetLemmatizer\n","\n","wl = WordNetLemmatizer()\n","wl_dataset = words_wo_puncts_stopwords\n","wl_dataset\n","\n","distinct_wl = []\n","\n","for w in wl_dataset:\n","  # print(w,\"-->\",wl.lemmatize(w))\n","  distinct_wl.append(wl.lemmatize(w))\n","\n","lemmawords =set(distinct_wl)\n","len(lemmawords)"],"metadata":{"id":"WZxMzKv4bMdl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693551106866,"user_tz":420,"elapsed":3100,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"df5da777-ae47-4922-8072-b068801de20d"},"id":"WZxMzKv4bMdl","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["35172"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":[],"metadata":{"id":"uMhXtA3RbMJk"},"id":"uMhXtA3RbMJk"},{"cell_type":"markdown","source":["### Exercise 2-6:\n","a) How many unique words are there once stemming is applied? (show the that performs the computation and outputs the result)\n","\n","b) How many unique words are there once lemmatization is applied? (show the code that performs the computation and outputs the result)"],"metadata":{"id":"LuWCQWX3bnsD"},"id":"LuWCQWX3bnsD"},{"cell_type":"markdown","source":["***Above Codes have found all the distinct words in Stem and Lemma***"],"metadata":{"id":"iNOz3E4xg3Sv"},"id":"iNOz3E4xg3Sv"},{"cell_type":"code","source":["print(\"Distinct Stem Words are:\",len(diststemwords))\n","print(\"Distinct Lemma Words are:\",len(lemmawords))\n"],"metadata":{"id":"N4WAh6UEbqNq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693551244612,"user_tz":420,"elapsed":5,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"0cd70360-8e88-49fb-f999-99b9634b7086"},"id":"N4WAh6UEbqNq","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Distinct Stem Words are: 26101\n","Distinct Lemma Words are: 35172\n"]}]},{"cell_type":"markdown","source":["## Part 3. Tokenization"],"metadata":{"id":"FQOoke3_bvbr"},"id":"FQOoke3_bvbr"},{"cell_type":"markdown","source":["### Exercise 3-1: Use the Penn Tree Bank tokenizer to tokenize the sentence\n","\n","---\n","\n","below\n","\n","Print the tokens that the tokenizer produces."],"metadata":{"id":"o0HA5ds8HL6-"},"id":"o0HA5ds8HL6-"},{"cell_type":"code","execution_count":13,"id":"204dbae3","metadata":{"id":"204dbae3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693725736763,"user_tz":420,"elapsed":155,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"4078aee0-5abb-43f7-c52a-578836ae47d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Please', 'pay', '$', '100.55', 'to', 'settle', 'your', 'bill.', 'Send', 'confirmation', 'to', 'confirm', '@', 'gmail.com', '.']\n"]}],"source":["from nltk.tokenize import TreebankWordTokenizer\n","s = 'Please pay $100.55 to settle your bill.  Send confirmation to confirm@gmail.com.'\n","\n","tokens = TreebankWordTokenizer().tokenize(s)\n","\n","# Print the tokens\n","print(tokens)"]},{"cell_type":"markdown","source":["## Part 4: Levenshtein Distance & Alignment\n","\n","Relevant nltk documentation: https://www.nltk.org/api/nltk.metrics.distance.html"],"metadata":{"id":"Tu9xug2Gxr84"},"id":"Tu9xug2Gxr84"},{"cell_type":"markdown","source":["### Exercise 4-1: Use the nltk functions edit_distance to compute the Levenshtein edit-distance between the strings \"intention\" and \"execution\""],"metadata":{"id":"fBsXnDQ-yPPE"},"id":"fBsXnDQ-yPPE"},{"cell_type":"code","source":["from nltk.metrics.distance import edit_distance\n","\n","w1 = \"intention\"\n","w2 = \"execution\"\n","\n","# help (edit_distance)\n","dist = edit_distance(w1,w2)\n","dist"],"metadata":{"id":"5aaSK4Ehylz7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693730073613,"user_tz":420,"elapsed":200,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"0cee8aef-ab94-4652-c2c5-6d38be2f2174"},"id":"5aaSK4Ehylz7","execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["### Exercise 4-2: Use the nltk function edit_distance_align to compute the minimum Levenshtein edit-distance based alignment mapping between the two strings \"intention\" and \"execution\""],"metadata":{"id":"NKWLhn1RzBGv"},"id":"NKWLhn1RzBGv"},{"cell_type":"code","source":["from nltk.metrics.distance import edit_distance_align\n","\n","w1 = \"intention\"\n","w2 = \"execution\"\n","\n","# help (edit_distance)\n","dist2 = edit_distance_align(w1,w2)\n","dist2"],"metadata":{"id":"Zc16veVuzBxM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693730301528,"user_tz":420,"elapsed":189,"user":{"displayName":"Gaurav Surtani","userId":"09066920757298032105"}},"outputId":"862e3708-c666-40b3-ba20-d1c8660d2e66"},"id":"Zc16veVuzBxM","execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 0),\n"," (1, 1),\n"," (2, 2),\n"," (3, 3),\n"," (4, 4),\n"," (5, 5),\n"," (6, 6),\n"," (7, 7),\n"," (8, 8),\n"," (9, 9)]"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":[],"metadata":{"id":"r-RgUsGl8Vgu"},"id":"r-RgUsGl8Vgu","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}