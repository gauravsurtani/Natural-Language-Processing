{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLnwYQ5M5qAw"
   },
   "source": [
    "## LDA example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6akQCA520TV"
   },
   "source": [
    "Notebook adapted from https://nbviewer.org/github/bmabey/pyLDAvis/blob/master/notebooks/sklearn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "v65UUo0018Vb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 8.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in d:\\anaconda\\lib\\site-packages (from pyLDAvis) (1.2.1)\n",
      "Collecting pandas>=2.0.0\n",
      "  Downloading pandas-2.1.0-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "     --------------------------------------- 11.1/11.1 MB 16.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numexpr in d:\\anaconda\\lib\\site-packages (from pyLDAvis) (2.8.4)\n",
      "Collecting funcy\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "     ------------------------------------- 302.2/302.2 kB 18.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: gensim in d:\\anaconda\\lib\\site-packages (from pyLDAvis) (4.3.0)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from pyLDAvis) (65.6.3)\n",
      "Collecting numpy>=1.24.2\n",
      "  Downloading numpy-1.26.0-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "     --------------------------------------- 15.8/15.8 MB 17.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from pyLDAvis) (1.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "     ------------------------------------- 341.8/341.8 kB 20.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2022.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\anaconda\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.1)\n",
      "Collecting pyfume\n",
      "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "     ---------------------------------------- 67.1/67.1 kB 3.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Collecting simpful\n",
      "  Downloading simpful-2.11.0-py3-none-any.whl (32 kB)\n",
      "Collecting fst-pso\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting miniful\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py): started\n",
      "  Building wheel for fst-pso (setup.py): finished with status 'done'\n",
      "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20448 sha256=3207a3ecad2ed9ece1817b850e1cd2eb8935b50b5e21bae17e576c572102edae\n",
      "  Stored in directory: c:\\users\\gsurt\\appdata\\local\\pip\\cache\\wheels\\01\\02\\ee\\df0699282986903a384b69aab4413af9efd26b3612b5dccc9e\n",
      "  Building wheel for miniful (setup.py): started\n",
      "  Building wheel for miniful (setup.py): finished with status 'done'\n",
      "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3522 sha256=109dc195829119892264bf46d54abfc54d8dedafc24b90c9891c2ca158e97aaf\n",
      "  Stored in directory: c:\\users\\gsurt\\appdata\\local\\pip\\cache\\wheels\\43\\aa\\48\\5c66b931ff013ad19774081aa19656637af5c0cc33b5494b30\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: funcy, tzdata, numpy, joblib, pandas, simpful, miniful, fst-pso, pyfume, FuzzyTM, pyLDAvis\n",
      "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 funcy-2.0 joblib-1.3.2 miniful-0.0.6 numpy-1.26.0 pandas-2.1.0 pyLDAvis-3.4.1 pyfume-0.2.25 simpful-2.11.0 tzdata-2023.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\gsurt\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "53VAEi5s11vb"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39menable_notebook()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pyLDAvis\n",
    "\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# Load 20 newsgroups dataset\n",
    "# First, the 20 newsgroups dataset available in sklearn is loaded. As always, the headers, footers and quotes are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqOLjPx340hN"
   },
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "docs_raw = newsgroups.data\n",
    "print(len(docs_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvabSTNK45Tp"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVDC3rvC2aKL"
   },
   "outputs": [],
   "source": [
    "# Convert to document-term matrix\n",
    "# Next, the raw documents are converted into document-term matrix, possibly as raw counts or in TF-IDF form.\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)\n",
    "dtm_tf = tf_vectorizer.fit_transform(docs_raw)\n",
    "print(dtm_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhBPBano2uIS"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(docs_raw)\n",
    "print(dtm_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGDhNAVY1KDQ"
   },
   "outputs": [],
   "source": [
    "# Fit Latent Dirichlet Allocation models - this step takes a while!\n",
    "\n",
    "# for TF DTM\n",
    "lda_tf = LatentDirichletAllocation(n_components=20, random_state=0)\n",
    "lda_tf.fit(dtm_tf)\n",
    "\n",
    "# for TFIDF DTM\n",
    "lda_tfidf = LatentDirichletAllocation(n_components=20, random_state=0)\n",
    "lda_tfidf.fit(dtm_tfidf)\n",
    "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "             evaluate_every=-1, learning_decay=0.7,\n",
    "             learning_method='online', learning_offset=10.0,\n",
    "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
    "             n_jobs=1, n_components=20, perp_tol=0.1, random_state=0,\n",
    "             topic_word_prior=None, total_samples=1000000.0, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBaQH6nH3ANm"
   },
   "outputs": [],
   "source": [
    "# Visualizing the models with pyLDAvis, using the term frequency (tf) vectors\n",
    "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpUEYQSS3YdR"
   },
   "outputs": [],
   "source": [
    "# Visualizing the models with pyLDAvis, using the tf-idf vectors\n",
    "\n",
    "pyLDAvis.sklearn.prepare(lda_tfidf, dtm_tfidf, tfidf_vectorizer)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
