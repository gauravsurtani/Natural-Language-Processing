{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e2b0e386",
      "metadata": {
        "id": "e2b0e386"
      },
      "source": [
        "# CMPE-259 Transformer Homework\n",
        "\n",
        "## Professor Jorjeta Jetcheva, T.A. Hardy Leung\n",
        "\n",
        "### Copyright 2023. All Right Reserved\n",
        "\n",
        "Welcome to the transformer assignment! This assignment is made of four parts. Part I, II, III are tutorials on tensors, PyTorch, and transformer. Part IV is the actual coding. You only need to write code for Part IV.\n",
        "\n",
        "The purpose of this assignment is to allow you to get as proficient with the transformer architecture as possible, starting only with decent Python coding skills and a basic understanding of deep learning at a higher level. You probably have _used_ or _adapted_ Keras or PyTorch codes in previous class projects starting with sample code, but it is not a requirement that you have _written_ your own PyTorch code.\n",
        "\n",
        "You will implement the transformer model on PyTorch almost from scratch, with a minimum amount of external dependencies other than the PyTorch framework and some pre-built modules. Since we are learning about natural language processing, you would have some more idea about embedding and the concept of auto-regressive next-token prediction. You need have a high-level understanding of how transformer works. If not, it is worth your time reading the seminal paper on transformer, \"*Attention is All You Need*\" by Vaswani et al. https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "Hopefully, by the end of this transformer assignment, you will have developed quite a bit of confidence and ability as a professional deep learning practioner or researcher, instead of getting stuck at copying or adapting others' code.\n",
        "\n",
        "It is strongly advised that you work on this assignment instead of looking up existing transformer implementation -- yes there are existing code out there, though some are very complicated. In fact, if you are not careful, you may end up spending even more time looking at existing code, and at the same time you rob yourself of a great learning opportunity.\n",
        "\n",
        "Part I and II will get you familiar with the basics of tensors and PyTorch. Don't skip it even if you think you know the materials. Some of them are quite advanced and necessary for transformer implementation.\n",
        "\n",
        "Part III talks about the mechanics of the transformer, mostly from an implementation perspective.\n",
        "\n",
        "You don't need to submit anything for Parts I, II, and III.\n",
        "\n",
        "Part IV is where the actual coding is. Search for the word `FIXME`, and make the appropriate changes. Part IV is further divided into many parts, as we will work on the building blocks towards the end-game of writing a fully functional transformer module. You'll be given instructions along the way. Verification functions are provided so that you know you have implemented each part correctly.\n",
        "\n",
        "Let's go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9383b913",
      "metadata": {
        "id": "9383b913"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e55315ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e55315ef",
        "outputId": "fdcc2cb2-95b1-44db-f9ae-5ffa52164dde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7aa63cadbb10>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "torch.manual_seed(42)  # this is to ensure deterministic behavior"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc536b44",
      "metadata": {
        "id": "dc536b44"
      },
      "source": [
        "---\n",
        "\n",
        "# [Exercise] Part 4 -- Transformer Implementation\n",
        "\n",
        "Now we have all the tools to implement transformer, as shown in the following diagram (from the paper \"Attention is All You Need\" https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58116e7c",
      "metadata": {
        "id": "58116e7c"
      },
      "source": [
        "---\n",
        "\n",
        "## 4.1 -- Positional Encoding\n",
        "\n",
        "Let's start with the positional encoding as a warm-up. You don't need to do anything here, but pay attention to how the information flows, as well as the shapes of the tensors along the way.\n",
        "\n",
        "This module takes the input which has been embedded in D-dimension per token, and there are `L` tokens (up to `maxL`). The shape of the tensor for a single input sequence therefore is therefore `[L, D]`, and according to the Transformer paper, there should be a matching positional encoding, also of shape `[L, D]`, whose elements should be defined as follows (paper section 3.5):\n",
        "\n",
        "![Screenshot%202023-10-03%20at%205.06.34%20PM.png](attachment:Screenshot%202023-10-03%20at%205.06.34%20PM.png)\n",
        "\n",
        "For your convenience, the following Python function can build such positional encoding for you, using matrix manipulation:\n",
        "\n",
        "```\n",
        "def positional_encoding_function(maxL, D):\n",
        "        pe = torch.zeros(maxL, D)\n",
        "            # pe measures (maxL, D)\n",
        "        position = torch.arange(0, maxL).unsqueeze(1)\n",
        "            # (maxL) --> (maxL, 1) via unsqueeze(1)\n",
        "        coeff = torch.exp(torch.arange(0, D, 2).float() * -(math.log(10000.0) / D))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * coeff)  # fill in even positions\n",
        "        pe[:, 1::2] = torch.cos(position * coeff)  # fill in odd positions\n",
        "        return pe\n",
        " ```\n",
        "\n",
        "Now, your job is to implement the module `PositionalEmbedding`, which simply take the input, and _add_ the output of the positional embedding to it, and return. Note that the input would be a tensor of shape `[B, L, D]`, where `B` is the batch size, `L` is the length of the each input token sequence. Note that all of the input token sequences must have the same length, with padding if necessary, and `L` must be less than or equal to `maxL` which is set when we construct the `PositionalEmbedding` object.\n",
        "\n",
        "To make this assignment as easy as possible, we provide a function called `verify_positional_encoding()` just so you would know if you get it right or not. Pay attention to the fact that the input length may be\n",
        "smaller than `maxL`. You may need to use the Python subrange functionalities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0cd09450",
      "metadata": {
        "id": "0cd09450"
      },
      "outputs": [],
      "source": [
        "def positional_encoding_function(maxL, D):\n",
        "        pe = torch.zeros(maxL, D)\n",
        "            # pe measures (maxL, D)\n",
        "        position = torch.arange(0, maxL).unsqueeze(1)\n",
        "            # (maxL) --> (maxL, 1) via unsqueeze(1)\n",
        "        coeff = torch.exp(torch.arange(0, D, 2).float() * -(math.log(10000.0) / D))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * coeff)  # fill in even positions\n",
        "        pe[:, 1::2] = torch.cos(position * coeff)  # fill in odd positions\n",
        "        return pe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "99047340",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99047340",
        "outputId": "2af99966-d190-4aa6-e8af-63068ff45438"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
              "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
              "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
              "        [-0.7568, -0.6536,  0.0400,  0.9992],\n",
              "        [-0.9589,  0.2837,  0.0500,  0.9988],\n",
              "        [-0.2794,  0.9602,  0.0600,  0.9982],\n",
              "        [ 0.6570,  0.7539,  0.0699,  0.9976],\n",
              "        [ 0.9894, -0.1455,  0.0799,  0.9968],\n",
              "        [ 0.4121, -0.9111,  0.0899,  0.9960]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "positional_encoding_function(10, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "431cb606",
      "metadata": {
        "id": "431cb606"
      },
      "outputs": [],
      "source": [
        "# FIXME: please implement this custom PyTorch model. To support batching,\n",
        "#        the shape of self.pe must be [1, maxL, D]\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, D, maxL):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.maxL = maxL\n",
        "        self.D = D\n",
        "        self.pe = positional_encoding_function(maxL, D).unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Ensure that the input length (L) is less than or equal to maxL\n",
        "      assert x.size(1) <= self.maxL, \"Input length (L) exceeds maxL\"\n",
        "\n",
        "      # Get the correct size for self.pe to match the length of x\n",
        "      pe_slice = self.pe[:, :x.size(1), :]\n",
        "\n",
        "      # Add positional encoding to the input\n",
        "      return x + pe_slice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positional_encoding = PositionalEncoding(4, 10)\n",
        "print(positional_encoding.pe.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu7VqlU9g4c5",
        "outputId": "6dcfba11-ba47-407b-c38f-5011b49b5cb0"
      },
      "id": "tu7VqlU9g4c5",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f7dd5b47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7dd5b47",
        "outputId": "32eca141-dec7-4e2c-ea54-9b9a46d7c77d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = tensor([[[0.2431, 0.4980, 0.7206, 0.3775],\n",
            "         [0.4099, 0.6627, 0.4661, 0.6243],\n",
            "         [0.0589, 0.3667, 0.1145, 0.1267],\n",
            "         [0.1336, 0.8447, 0.0353, 0.6310],\n",
            "         [0.4305, 0.3908, 0.7980, 0.1252]],\n",
            "\n",
            "        [[0.7211, 0.7129, 0.1923, 0.6771],\n",
            "         [0.4786, 0.1531, 0.0267, 0.5136],\n",
            "         [0.1609, 0.2147, 0.3886, 0.6307],\n",
            "         [0.0440, 0.2393, 0.9905, 0.3157],\n",
            "         [0.3681, 0.7550, 0.4471, 0.2478]],\n",
            "\n",
            "        [[0.2217, 0.3223, 0.1107, 0.5803],\n",
            "         [0.0943, 0.3119, 0.4668, 0.4528],\n",
            "         [0.9580, 0.6907, 0.6251, 0.5495],\n",
            "         [0.3926, 0.9498, 0.2189, 0.0112],\n",
            "         [0.5274, 0.9410, 0.9193, 0.1334]]])\n",
            "input.shape = torch.Size([3, 5, 4])\n",
            "----------------------------------------------------------------\n",
            "output = tensor([[[ 0.2431,  1.4980,  0.7206,  1.3775],\n",
            "         [ 1.2514,  1.2030,  0.4761,  1.6242],\n",
            "         [ 0.9682, -0.0494,  0.1345,  1.1265],\n",
            "         [ 0.2747, -0.1453,  0.0653,  1.6306],\n",
            "         [-0.3263, -0.2628,  0.8380,  1.1244]],\n",
            "\n",
            "        [[ 0.7211,  1.7129,  0.1923,  1.6771],\n",
            "         [ 1.3201,  0.6934,  0.0367,  1.5136],\n",
            "         [ 1.0702, -0.2014,  0.4086,  1.6305],\n",
            "         [ 0.1851, -0.7507,  1.0205,  1.3153],\n",
            "         [-0.3887,  0.1014,  0.4871,  1.2470]],\n",
            "\n",
            "        [[ 0.2217,  1.3223,  0.1107,  1.5803],\n",
            "         [ 0.9358,  0.8522,  0.4768,  1.4527],\n",
            "         [ 1.8673,  0.2746,  0.6451,  1.5493],\n",
            "         [ 0.5337, -0.0402,  0.2489,  1.0108],\n",
            "         [-0.2294,  0.2874,  0.9593,  1.1326]]])\n",
            "output.shape = torch.Size([3, 5, 4])\n",
            "----------------------------------------------------------------\n",
            "expected_output = tensor([[[ 0.2431,  1.4980,  0.7206,  1.3775],\n",
            "         [ 1.2514,  1.2030,  0.4761,  1.6242],\n",
            "         [ 0.9682, -0.0494,  0.1345,  1.1265],\n",
            "         [ 0.2747, -0.1453,  0.0653,  1.6306],\n",
            "         [-0.3263, -0.2628,  0.8380,  1.1244]],\n",
            "\n",
            "        [[ 0.7211,  1.7129,  0.1923,  1.6771],\n",
            "         [ 1.3201,  0.6934,  0.0367,  1.5136],\n",
            "         [ 1.0702, -0.2014,  0.4086,  1.6305],\n",
            "         [ 0.1851, -0.7507,  1.0205,  1.3153],\n",
            "         [-0.3887,  0.1014,  0.4871,  1.2470]],\n",
            "\n",
            "        [[ 0.2217,  1.3223,  0.1107,  1.5803],\n",
            "         [ 0.9358,  0.8522,  0.4768,  1.4527],\n",
            "         [ 1.8673,  0.2746,  0.6451,  1.5493],\n",
            "         [ 0.5337, -0.0402,  0.2489,  1.0108],\n",
            "         [-0.2294,  0.2874,  0.9593,  1.1326]]])\n",
            "expected_output.shape = torch.Size([3, 5, 4])\n",
            "----------------------------------------------------------------\n",
            "** passed verify_positional_encoding() **\n"
          ]
        }
      ],
      "source": [
        "def verify_positional_encoding():\n",
        "    d = 4\n",
        "    L = 10\n",
        "    positional_encoding = PositionalEncoding(d, L)\n",
        "\n",
        "    if positional_encoding.pe.shape != torch.Size([1, L, d]):\n",
        "        print(\"** failed (unexpected self.pe shape) **\")\n",
        "        return\n",
        "\n",
        "    input = torch.tensor([[[0.2431, 0.4980, 0.7206, 0.3775],\n",
        "         [0.4099, 0.6627, 0.4661, 0.6243],\n",
        "         [0.0589, 0.3667, 0.1145, 0.1267],\n",
        "         [0.1336, 0.8447, 0.0353, 0.6310],\n",
        "         [0.4305, 0.3908, 0.7980, 0.1252]],\n",
        "\n",
        "        [[0.7211, 0.7129, 0.1923, 0.6771],\n",
        "         [0.4786, 0.1531, 0.0267, 0.5136],\n",
        "         [0.1609, 0.2147, 0.3886, 0.6307],\n",
        "         [0.0440, 0.2393, 0.9905, 0.3157],\n",
        "         [0.3681, 0.7550, 0.4471, 0.2478]],\n",
        "\n",
        "        [[0.2217, 0.3223, 0.1107, 0.5803],\n",
        "         [0.0943, 0.3119, 0.4668, 0.4528],\n",
        "         [0.9580, 0.6907, 0.6251, 0.5495],\n",
        "         [0.3926, 0.9498, 0.2189, 0.0112],\n",
        "         [0.5274, 0.9410, 0.9193, 0.1334]]])\n",
        "    expected_output = torch.tensor([[[ 0.2431,  1.4980,  0.7206,  1.3775],\n",
        "         [ 1.2514,  1.2030,  0.4761,  1.6242],\n",
        "         [ 0.9682, -0.0494,  0.1345,  1.1265],\n",
        "         [ 0.2747, -0.1453,  0.0653,  1.6306],\n",
        "         [-0.3263, -0.2628,  0.8380,  1.1244]],\n",
        "\n",
        "        [[ 0.7211,  1.7129,  0.1923,  1.6771],\n",
        "         [ 1.3201,  0.6934,  0.0367,  1.5136],\n",
        "         [ 1.0702, -0.2014,  0.4086,  1.6305],\n",
        "         [ 0.1851, -0.7507,  1.0205,  1.3153],\n",
        "         [-0.3887,  0.1014,  0.4871,  1.2470]],\n",
        "\n",
        "        [[ 0.2217,  1.3223,  0.1107,  1.5803],\n",
        "         [ 0.9358,  0.8522,  0.4768,  1.4527],\n",
        "         [ 1.8673,  0.2746,  0.6451,  1.5493],\n",
        "         [ 0.5337, -0.0402,  0.2489,  1.0108],\n",
        "         [-0.2294,  0.2874,  0.9593,  1.1326]]])\n",
        "\n",
        "    print(f'input = {input}')\n",
        "    print(f'input.shape = {input.shape}')\n",
        "    print('----------------------------------------------------------------')\n",
        "    output = positional_encoding(input)\n",
        "    print(f'output = {output}')\n",
        "    print(f'output.shape = {output.shape}')\n",
        "    print('----------------------------------------------------------------')\n",
        "    print(f'expected_output = {expected_output}')\n",
        "    print(f'expected_output.shape = {expected_output.shape}')\n",
        "    print('----------------------------------------------------------------')\n",
        "    if output.shape != expected_output.shape or \\\n",
        "            torch.max(torch.abs(output - expected_output)) > 1e-04:\n",
        "        print(f'         output.shape = {output.shape}')\n",
        "        print(f'               output = {output}')\n",
        "        print(f'expected_output.shape = {expected_output.shape}')\n",
        "        print(f'      expected_output = {expected_output}')\n",
        "        print(\"** failed (mismatched output) **\")\n",
        "        return\n",
        "\n",
        "    print(\"** passed verify_positional_encoding() **\")\n",
        "\n",
        "verify_positional_encoding()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a854e69",
      "metadata": {
        "id": "7a854e69"
      },
      "source": [
        "---\n",
        "\n",
        "## [Exercise] 4.2 -- Feed-Forward\n",
        "\n",
        "Another module that is described in the Attention paper is a feed-forward module (section 3.3) -- it is made of \"two linear transformations with a ReLU activation inbetween\".\n",
        "\n",
        "It would be good if you can translate this directly into code. If not, here is the flow:\n",
        "\n",
        "```\n",
        "input --> linear(D, D_ff) --> ReLU   --> linear(D_ff, D) --> output\n",
        "          (fc1)               (relu)     (fc2)\n",
        "```\n",
        "\n",
        "Also make sure you can handle batches, i.e. you would expect the input to have a shape that is `[B, L, D]`. Note that `D` and `D_ff` will be provided as input during the construction of the feed-forward object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "58cdbe78",
      "metadata": {
        "id": "58cdbe78"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, D, D_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(D, D_ff)  # Linear transformation 1\n",
        "        self.fc2 = nn.Linear(D_ff, D)  # Linear transformation 2\n",
        "        self.relu = nn.ReLU()          # ReLU activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)   # Apply first linear transformation\n",
        "        out = self.relu(out)  # Apply ReLU activation\n",
        "        out = self.fc2(out)   # Apply second linear transformation\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "01341239",
      "metadata": {
        "scrolled": false,
        "id": "01341239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc4af6b0-df53-478b-e8ec-bd988b82d9b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = tensor([[[0.2431, 0.4980, 0.7206, 0.3775],\n",
            "         [0.4099, 0.6627, 0.4661, 0.6243],\n",
            "         [0.0589, 0.3667, 0.1145, 0.1267],\n",
            "         [0.1336, 0.8447, 0.0353, 0.6310],\n",
            "         [0.4305, 0.3908, 0.7980, 0.1252]],\n",
            "\n",
            "        [[0.7211, 0.7129, 0.1923, 0.6771],\n",
            "         [0.4786, 0.1531, 0.0267, 0.5136],\n",
            "         [0.1609, 0.2147, 0.3886, 0.6307],\n",
            "         [0.0440, 0.2393, 0.9905, 0.3157],\n",
            "         [0.3681, 0.7550, 0.4471, 0.2478]],\n",
            "\n",
            "        [[0.2217, 0.3223, 0.1107, 0.5803],\n",
            "         [0.0943, 0.3119, 0.4668, 0.4528],\n",
            "         [0.9580, 0.6907, 0.6251, 0.5495],\n",
            "         [0.3926, 0.9498, 0.2189, 0.0112],\n",
            "         [0.5274, 0.9410, 0.9193, 0.1334]]])\n",
            "input.shape = torch.Size([3, 5, 4])\n",
            "----------------------------------------------------------------\n",
            "output = tensor([[[-0.0597,  0.0945,  0.0395, -0.2518],\n",
            "         [-0.1087,  0.1272,  0.0404, -0.1591],\n",
            "         [-0.0683,  0.0398, -0.0016, -0.1937],\n",
            "         [-0.1594,  0.1681, -0.0724, -0.1056],\n",
            "         [-0.0407,  0.0176,  0.0667, -0.2978]],\n",
            "\n",
            "        [[-0.1535,  0.1135,  0.0863, -0.0775],\n",
            "         [ 0.0179,  0.0418,  0.1095, -0.0774],\n",
            "         [ 0.0021,  0.1400,  0.0694, -0.1814],\n",
            "         [-0.0618,  0.0594,  0.1050, -0.2809],\n",
            "         [-0.1225,  0.0564, -0.0130, -0.2060]],\n",
            "\n",
            "        [[-0.0257,  0.1084,  0.0363, -0.1250],\n",
            "         [-0.0230,  0.1223,  0.0449, -0.2151],\n",
            "         [-0.1516,  0.0530,  0.1148, -0.1779],\n",
            "         [-0.1689,  0.0216, -0.0733, -0.1875],\n",
            "         [-0.1763,  0.0052, -0.0145, -0.3051]]], grad_fn=<ViewBackward0>)\n",
            "output.shape = torch.Size([3, 5, 4])\n",
            "----------------------------------------------------------------\n",
            "expected_output = tensor([[[-0.0597,  0.0945,  0.0395, -0.2518],\n",
            "         [-0.1087,  0.1272,  0.0404, -0.1591],\n",
            "         [-0.0683,  0.0398, -0.0016, -0.1937],\n",
            "         [-0.1594,  0.1681, -0.0724, -0.1056],\n",
            "         [-0.0407,  0.0176,  0.0667, -0.2978]],\n",
            "\n",
            "        [[-0.1535,  0.1135,  0.0863, -0.0775],\n",
            "         [ 0.0179,  0.0418,  0.1095, -0.0774],\n",
            "         [ 0.0021,  0.1400,  0.0694, -0.1814],\n",
            "         [-0.0618,  0.0594,  0.1050, -0.2809],\n",
            "         [-0.1225,  0.0564, -0.0130, -0.2060]],\n",
            "\n",
            "        [[-0.0257,  0.1084,  0.0363, -0.1250],\n",
            "         [-0.0230,  0.1223,  0.0449, -0.2151],\n",
            "         [-0.1516,  0.0530,  0.1148, -0.1779],\n",
            "         [-0.1689,  0.0216, -0.0733, -0.1875],\n",
            "         [-0.1763,  0.0052, -0.0145, -0.3051]]])\n",
            "expected_output.shape = torch.Size([3, 5, 4])\n",
            "----------------------------------------------------------------\n",
            "** passed verify_feed_forward() **\n"
          ]
        }
      ],
      "source": [
        "def verify_feed_forward():\n",
        "    D = 4\n",
        "    D_ff = 20\n",
        "    L = 10\n",
        "    feed_forward = FeedForward(D, D_ff)\n",
        "\n",
        "    input = torch.tensor([[[0.2431, 0.4980, 0.7206, 0.3775],\n",
        "         [0.4099, 0.6627, 0.4661, 0.6243],\n",
        "         [0.0589, 0.3667, 0.1145, 0.1267],\n",
        "         [0.1336, 0.8447, 0.0353, 0.6310],\n",
        "         [0.4305, 0.3908, 0.7980, 0.1252]],\n",
        "\n",
        "        [[0.7211, 0.7129, 0.1923, 0.6771],\n",
        "         [0.4786, 0.1531, 0.0267, 0.5136],\n",
        "         [0.1609, 0.2147, 0.3886, 0.6307],\n",
        "         [0.0440, 0.2393, 0.9905, 0.3157],\n",
        "         [0.3681, 0.7550, 0.4471, 0.2478]],\n",
        "\n",
        "        [[0.2217, 0.3223, 0.1107, 0.5803],\n",
        "         [0.0943, 0.3119, 0.4668, 0.4528],\n",
        "         [0.9580, 0.6907, 0.6251, 0.5495],\n",
        "         [0.3926, 0.9498, 0.2189, 0.0112],\n",
        "         [0.5274, 0.9410, 0.9193, 0.1334]]])\n",
        "    expected_output = torch.tensor([[[-0.0597,  0.0945,  0.0395, -0.2518],\n",
        "         [-0.1087,  0.1272,  0.0404, -0.1591],\n",
        "         [-0.0683,  0.0398, -0.0016, -0.1937],\n",
        "         [-0.1594,  0.1681, -0.0724, -0.1056],\n",
        "         [-0.0407,  0.0176,  0.0667, -0.2978]],\n",
        "\n",
        "        [[-0.1535,  0.1135,  0.0863, -0.0775],\n",
        "         [ 0.0179,  0.0418,  0.1095, -0.0774],\n",
        "         [ 0.0021,  0.1400,  0.0694, -0.1814],\n",
        "         [-0.0618,  0.0594,  0.1050, -0.2809],\n",
        "         [-0.1225,  0.0564, -0.0130, -0.2060]],\n",
        "\n",
        "        [[-0.0257,  0.1084,  0.0363, -0.1250],\n",
        "         [-0.0230,  0.1223,  0.0449, -0.2151],\n",
        "         [-0.1516,  0.0530,  0.1148, -0.1779],\n",
        "         [-0.1689,  0.0216, -0.0733, -0.1875],\n",
        "         [-0.1763,  0.0052, -0.0145, -0.3051]]])\n",
        "\n",
        "    print(f'input = {input}')\n",
        "    print(f'input.shape = {input.shape}')\n",
        "    print('----------------------------------------------------------------')\n",
        "    output = feed_forward(input)\n",
        "    print(f'output = {output}')\n",
        "    print(f'output.shape = {output.shape}')\n",
        "    print('----------------------------------------------------------------')\n",
        "    print(f'expected_output = {expected_output}')\n",
        "    print(f'expected_output.shape = {expected_output.shape}')\n",
        "    print('----------------------------------------------------------------')\n",
        "    if output.shape != expected_output.shape or \\\n",
        "            torch.max(torch.abs(output - expected_output)) > 1e-04:\n",
        "        print(f'         output.shape = {output.shape}')\n",
        "        print(f'               output = {output}')\n",
        "        print(f'expected_output.shape = {expected_output.shape}')\n",
        "        print(f'      expected_output = {expected_output}')\n",
        "        print(\"** failed (mismatched output) **\")\n",
        "        return\n",
        "\n",
        "    print(\"** passed verify_feed_forward() **\")\n",
        "\n",
        "verify_feed_forward()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65ab440d",
      "metadata": {
        "id": "65ab440d"
      },
      "source": [
        "---\n",
        "\n",
        "## [Exercise] 4.3 -- Multi-Head Splitting and Combining\n",
        "\n",
        "You heard about the term \"multi-head\" attention but do you know how it works? Conceptually, let's say we have a tensor that represents a token sequence of length `L` and dimension `D`. For example (for illustration only),\n",
        "the following tensor represents a sequence of 12 tokens, with 6 dimensions each. The shape of the tensor is `[12, 6]`:\n",
        "\n",
        "```\n",
        "tensor([[ 0,  1,  2,  3,  4,  5],\n",
        "        [ 6,  7,  8,  9, 10, 11],\n",
        "        [12, 13, 14, 15, 16, 17],\n",
        "        [18, 19, 20, 21, 22, 23],\n",
        "        [24, 25, 26, 27, 28, 29],\n",
        "        [30, 31, 32, 33, 34, 35],\n",
        "        [36, 37, 38, 39, 40, 41],\n",
        "        [42, 43, 44, 45, 46, 47],\n",
        "        [48, 49, 50, 51, 52, 53],\n",
        "        [54, 55, 56, 57, 58, 59],\n",
        "        [60, 61, 62, 63, 64, 65],\n",
        "        [66, 67, 68, 69, 70, 71]])\n",
        "```\n",
        "\n",
        "Many people misunderstood multi-head attention as merely installing multiple copies of the attention mechanism at the expense of higher memory and runtime requirement. That is not correct. Instead, the actual implementation of multi-head attention is to chop the tensor along the dimensions into multiple heads. For example, we can chop the above sequence into 3 heads. So instead of a tensor of shape `[12, 6]`, we have three tensors of shape `[12, 2]`. Moreover, you can look at the three tensors of shape `[12, 2]` as one tensor of shape `[3, 12, 2]`:\n",
        "\n",
        "```\n",
        "tensor([[[ 0,  1],\n",
        "         [ 6,  7],\n",
        "         [12, 13],\n",
        "         [18, 19],\n",
        "         [24, 25],\n",
        "         [30, 31],\n",
        "         [36, 37],\n",
        "         [42, 43],\n",
        "         [48, 49],\n",
        "         [54, 55],\n",
        "         [60, 61],\n",
        "         [66, 67]],\n",
        "\n",
        "        [[ 2,  3],\n",
        "         [ 8,  9],\n",
        "         [14, 15],\n",
        "         [20, 21],\n",
        "         [26, 27],\n",
        "         [32, 33],\n",
        "         [38, 39],\n",
        "         [44, 45],\n",
        "         [50, 51],\n",
        "         [56, 57],\n",
        "         [62, 63],\n",
        "         [68, 69]],\n",
        "\n",
        "        [[ 4,  5],\n",
        "         [10, 11],\n",
        "         [16, 17],\n",
        "         [22, 23],\n",
        "         [28, 29],\n",
        "         [34, 35],\n",
        "         [40, 41],\n",
        "         [46, 47],\n",
        "         [52, 53],\n",
        "         [58, 59],\n",
        "         [64, 65],\n",
        "         [70, 71]]])\n",
        "```\n",
        "\n",
        "Does it make sense? Each of the three \"heads\" is a sequence of length `L`, each with one third of the original dimensions (which is required to be evenly divisible by the number of heads).\n",
        "\n",
        "How do you do this? Note that this can be accomplished with some PyTorch tensor manipulation as shown before, but it is not as easy as calling `reshape()`, because the ordering of elements would be wrong.\n",
        "\n",
        "In the following, you need to implement two functions, `split_heads()` and `combine_heads()` to prepare the data for the multi-head attention mechanism. Both functions need to support batching, namely the input is not just a tensor, but a batch of tensors. More specifically,\n",
        "\n",
        "`split_heads()` -- this functions takes in a tensor of shape `[B, L, D]`, and a parameter called `H`, where `B` is the batch size, `L` is the length of the token sequence, `D` is the dimension of each token, and return a tensor of shape `[B, H, L, Dh]`, where `Dh` is `D` divided by `h` (integer division). For example, you could start with a batch of 16 sequences, each of which is 1024 tokens long, and each token has an embedding dimension of 256. You call `split_heads()` to split the tensor into 8 heads. The result is a batch of 16 8-header tensors, each of which is 1024 tokens long, and each token has an embedding dimension of 32:\n",
        "\n",
        "```\n",
        "[16, 1024, 256] --> [16, 8, 1024, 32]\n",
        "```\n",
        "\n",
        "`combine_heads()` -- this function reverses the process of `split_heads()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7e43c9c0",
      "metadata": {
        "id": "7e43c9c0"
      },
      "outputs": [],
      "source": [
        "def split_heads(input, H):\n",
        "    B, L, D = input.shape\n",
        "    Dh = D // H  # Calculate the dimension of each head\n",
        "    input = input.view(B, L, H, Dh)  # Reshape the tensor\n",
        "    return input.permute(0, 2, 1, 3)  # Permute dimensions for correct shape\n",
        "\n",
        "def combine_heads(input):\n",
        "    B, H, L, Dh = input.shape\n",
        "    D = H * Dh  # Calculate the original dimension\n",
        "    input = input.permute(0, 2, 1, 3)  # Permute dimensions\n",
        "    return input.contiguous().view(B, L, D)  # Reshape the tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a04bc889",
      "metadata": {
        "id": "a04bc889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "464307d7-9633-46ff-a0e8-f6c36ae3ebab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calling split_heads(input, 3), where input.shape = [3, 4, 6] ...\n",
            "calling combine_heads(input), where input.shape = [3, 3, 4, 2] ...\n",
            "----------------------------------------------------------------\n",
            "** passed verify_split_combine_heads() **\n"
          ]
        }
      ],
      "source": [
        "def verify_split_combine_heads():\n",
        "    A = torch.tensor([[[ 0,  1,  2,  3,  4,  5],\n",
        "         [ 6,  7,  8,  9, 10, 11],\n",
        "         [12, 13, 14, 15, 16, 17],\n",
        "         [18, 19, 20, 21, 22, 23]],\n",
        "\n",
        "        [[24, 25, 26, 27, 28, 29],\n",
        "         [30, 31, 32, 33, 34, 35],\n",
        "         [36, 37, 38, 39, 40, 41],\n",
        "         [42, 43, 44, 45, 46, 47]],\n",
        "\n",
        "        [[48, 49, 50, 51, 52, 53],\n",
        "         [54, 55, 56, 57, 58, 59],\n",
        "         [60, 61, 62, 63, 64, 65],\n",
        "         [66, 67, 68, 69, 70, 71]]])\n",
        "\n",
        "    B = torch.tensor([[[[ 0,  1],\n",
        "          [ 6,  7],\n",
        "          [12, 13],\n",
        "          [18, 19]],\n",
        "\n",
        "         [[ 2,  3],\n",
        "          [ 8,  9],\n",
        "          [14, 15],\n",
        "          [20, 21]],\n",
        "\n",
        "         [[ 4,  5],\n",
        "          [10, 11],\n",
        "          [16, 17],\n",
        "          [22, 23]]],\n",
        "\n",
        "\n",
        "        [[[24, 25],\n",
        "          [30, 31],\n",
        "          [36, 37],\n",
        "          [42, 43]],\n",
        "\n",
        "         [[26, 27],\n",
        "          [32, 33],\n",
        "          [38, 39],\n",
        "          [44, 45]],\n",
        "\n",
        "         [[28, 29],\n",
        "          [34, 35],\n",
        "          [40, 41],\n",
        "          [46, 47]]],\n",
        "\n",
        "\n",
        "        [[[48, 49],\n",
        "          [54, 55],\n",
        "          [60, 61],\n",
        "          [66, 67]],\n",
        "\n",
        "         [[50, 51],\n",
        "          [56, 57],\n",
        "          [62, 63],\n",
        "          [68, 69]],\n",
        "\n",
        "         [[52, 53],\n",
        "          [58, 59],\n",
        "          [64, 65],\n",
        "          [70, 71]]]])\n",
        "\n",
        "    print(\"calling split_heads(input, 3), where input.shape = [3, 4, 6] ...\")\n",
        "    BB = split_heads(A, 3)\n",
        "\n",
        "    print(\"calling combine_heads(input), where input.shape = [3, 3, 4, 2] ...\")\n",
        "    AA = combine_heads(B)\n",
        "\n",
        "    if not torch.equal(B, BB):\n",
        "        print(\"** failed split_heads() **\")\n",
        "        return\n",
        "    if not torch.equal(A, AA):\n",
        "        print(\"** failed combine_heads() **\")\n",
        "        return\n",
        "\n",
        "    print('----------------------------------------------------------------')\n",
        "    print(\"** passed verify_split_combine_heads() **\")\n",
        "\n",
        "verify_split_combine_heads()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf6ade2",
      "metadata": {
        "id": "9bf6ade2"
      },
      "source": [
        "---\n",
        "\n",
        "## [Exercise] 4.4 -- Masking\n",
        "\n",
        "The word `mask` appeared only twice in the \"Attention is All You Need\" paper, but is a significant concept in the transformer implementation, true to the saying that the devil is in the details. We decided to simply give you the full explanation, and provide the functions to generate the masks, so you don't need to do any work (yay!). For completeness, it is good to understand how masking works.\n",
        "\n",
        "Let's start with the key equation that describes the inner working of transformer:\n",
        "\n",
        "![Screenshot%202023-10-07%20at%209.04.21%20PM.png](attachment:Screenshot%202023-10-07%20at%209.04.21%20PM.png)\n",
        "\n",
        "We had explained this concept before. You have a query tensor `Q`, a key tensor `K`, and a value tensor `V`.\n",
        "The shapes of the tensors are `[B, L_q, D]`, `[B, L_kv, D]`, and `[B, L_kv, D]` respectively, for some\n",
        "`L_q` and `L_kv` in the second dimension, and `D` in the third dimension. `B` is the size of the\n",
        "batch, which for the purpose of this discussion, can be treated as a constant. Note that the query and the key tensors can have different shapes -- going back to Part III for more details.\n",
        "\n",
        "The first attention module in the decoder requires special attention in order to ensure we don't peek into the unknown future. Think about how, during inference, the target tensor is incrementally constructed. Therefore, we need a mask to mask out all query-key pairs that are illegal (by setting the score to negative infinity). More\n",
        "formally, we want a mask of shape `[L_q, L_k]` such that `mask[q, k] = 1` if this query-key pair is legal, and `mask[q, k] = 0` otherwise.\n",
        "\n",
        "\n",
        "Let's take a look at all three attentions:\n",
        "\n",
        "(1) encoder self-attention -- since we know the entire sequence of input already, there is no need to mask out anything. In transformer-speak, a token can attend to another token in the future, since we are talking about the source sequence.\n",
        "\n",
        "(2) decoder self-attention -- note that the query and the key are from the same tensor, i.e. of shape `[L_q, L_q]`. The mask should be set such that `mask[q, k] = 1` if `q >= k`, and `mask[q, k] = 0` otherwise. Here, `1` is legal, and `0` is illegal. This can be achieved by:\n",
        "\n",
        "```\n",
        "def generate_causality_mask(target, debug=False):\n",
        "    L = target.size(1)\n",
        "    causality_mask = torch.tril(torch.ones(1, L, L), diagonal=0).int()\n",
        "    return causality_mask\n",
        "```\n",
        "\n",
        "(3) decoder cross-attention -- there is no need for masking because the source tensor is known.\n",
        "\n",
        "Later on, you can then use the function to mask out entries deemed illegal by replacing the illegal\n",
        "entries with negative infinity (in practice, we use `-1e9`) using the torch function `masked_fill()`\n",
        "(see an example below).\n",
        "\n",
        "Note that, for tensor shape compatibility reason, the function that generate the causality mask\n",
        "returns a tensor with a shape `[1, L, L]` instead of `[L, L]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6d54c354",
      "metadata": {
        "id": "6d54c354"
      },
      "outputs": [],
      "source": [
        "def generate_causality_mask(target):\n",
        "    L = target.size(1)\n",
        "    return torch.tril(torch.ones(1, L, L), diagonal=0).int()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b9a2ebf9",
      "metadata": {
        "id": "b9a2ebf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c8e2cb6-4d29-4733-fa14-910d1e2c5269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "causality_mask = tensor([[[1, 0, 0, 0, 0],\n",
            "         [1, 1, 0, 0, 0],\n",
            "         [1, 1, 1, 0, 0],\n",
            "         [1, 1, 1, 1, 0],\n",
            "         [1, 1, 1, 1, 1]]], dtype=torch.int32)\n",
            "\n",
            "causality_mask.shape = torch.Size([1, 5, 5])\n",
            "----------------------------------------------------------------\n",
            "another_t = tensor([[[ 0,  1,  2,  3,  4],\n",
            "         [ 5,  6,  7,  8,  9],\n",
            "         [10, 11, 12, 13, 14],\n",
            "         [15, 16, 17, 18, 19],\n",
            "         [20, 21, 22, 23, 24]]])\n",
            "\n",
            "another_t.shape = torch.Size([1, 5, 5])\n",
            "----------------------------------------------------------------\n",
            "another_t_with_masking = tensor([[[          0, -1000000000, -1000000000, -1000000000, -1000000000],\n",
            "         [          5,           6, -1000000000, -1000000000, -1000000000],\n",
            "         [         10,          11,          12, -1000000000, -1000000000],\n",
            "         [         15,          16,          17,          18, -1000000000],\n",
            "         [         20,          21,          22,          23,          24]]])\n",
            "\n",
            "another_t_with_masking.shape = torch.Size([1, 5, 5])\n"
          ]
        }
      ],
      "source": [
        "t = torch.tensor([[1.5, 0.2, 4.5, -1.2, 2.0]])\n",
        "\n",
        "causality_mask = generate_causality_mask(t)\n",
        "print(f'causality_mask = {causality_mask}')\n",
        "print()\n",
        "print(f'causality_mask.shape = {causality_mask.shape}')\n",
        "print('----------------------------------------------------------------')\n",
        "\n",
        "another_t = torch.arange(0, 25).reshape([1, 5, 5])\n",
        "print(f'another_t = {another_t}')\n",
        "print()\n",
        "print(f'another_t.shape = {another_t.shape}')\n",
        "print('----------------------------------------------------------------')\n",
        "another_t_with_masking = another_t.masked_fill(causality_mask == 0, -1e9)\n",
        "print(f'another_t_with_masking = {another_t_with_masking}')\n",
        "print()\n",
        "print(f'another_t_with_masking.shape = {another_t_with_masking.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a7392489",
      "metadata": {
        "id": "a7392489",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e89c1f9-2f60-4e5c-9c91-da4fe4f7d4fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "** passed verify_generate_causality_mask() **\n"
          ]
        }
      ],
      "source": [
        "def verify_generate_causality_mask():\n",
        "    t = torch.tensor([[1.4, 1.3, 2.5, 0.1, -1.3, 0.0]])\n",
        "    causality_mask = generate_causality_mask(t)\n",
        "    expected_causality_mask = torch.tensor([[[1, 0, 0, 0, 0, 0],\n",
        "        [1, 1, 0, 0, 0, 0],\n",
        "        [1, 1, 1, 0, 0, 0],\n",
        "        [1, 1, 1, 1, 0, 0],\n",
        "        [1, 1, 1, 1, 1, 0],\n",
        "        [1, 1, 1, 1, 1, 1]]], dtype=torch.int32)\n",
        "    if not torch.equal(causality_mask, expected_causality_mask):\n",
        "        print(\"** failed generate_causality_mask() **\")\n",
        "        return\n",
        "\n",
        "    print('----------------------------------------------------------------')\n",
        "    print(\"** passed verify_generate_causality_mask() **\")\n",
        "\n",
        "verify_generate_causality_mask()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3157d74e",
      "metadata": {
        "id": "3157d74e"
      },
      "source": [
        "![Screenshot%202023-10-04%20at%203.33.32%20PM.png](attachment:Screenshot%202023-10-04%20at%203.33.32%20PM.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f6fd2f4",
      "metadata": {
        "id": "2f6fd2f4"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "## [Exercise] 4.5 -- Scaled Dot-Product Attention\n",
        "\n",
        "![Screenshot%202023-10-07%20at%209.04.21%20PM.png](attachment:Screenshot%202023-10-07%20at%209.04.21%20PM.png)\n",
        "\n",
        "\n",
        "You shall now write an important function (not a module yet) called `scaled_dot_product_attention()`,\n",
        "which takes in three tensors `Q`, `K`, `V`, an optional parameter called `mask`, and a dimension\n",
        "`d_k`. This function should return a tensor that computes the equation set forth in section 3.2.1:\n",
        "\n",
        "Conceptually, each of the tensors `Q`, `K`, `V` is a 2D matrix (of shapes `[L_q, D]`, `[L_k, D]`, and\n",
        "`[L_v, D]` respectively).\n",
        "However, in order to support batching and multi-heads, each of them is shaped as a 4-dimensional tensor with shape `[B, H, L_*, D]`, where `B` is the batch size, `H` is the number of heads, and `L_*` is the length of the input sequence.\n",
        "\n",
        "This function should be completed in 4 steps:\n",
        "\n",
        "(1) compute the matrix product of `Q` and transposed `K`, divided by the square root of `d_k`. We shall call the result `attn_scores`.\n",
        "\n",
        "(2) if `mask` is defined (i.e. not `None`), `attn_scores` needs to be masked filled with negative infinity\n",
        "if the mask entry is zero.\n",
        "\n",
        "(3) compute `attn_probabilities` which is the softmax of `attn_scores` in the last dimension.\n",
        "\n",
        "(4) compute `output` which is the matrix product of `attn_probabilities` and the tensor `V`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "147f8890",
      "metadata": {
        "id": "147f8890"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(Q, K, V, d_k, mask=None, negative_infinity=-1e9):\n",
        "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)\n",
        "\n",
        "    if mask is not None:\n",
        "        attn_scores = attn_scores.masked_fill(mask == 0, negative_infinity)\n",
        "\n",
        "    attn_probabilities = torch.nn.functional.softmax(attn_scores, dim=-1)\n",
        "    output = torch.matmul(attn_probabilities, V)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1c3da857",
      "metadata": {
        "id": "1c3da857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "663ac600-ce7b-424a-9de2-7672f02687a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "** passed verify_scaled_dot_product_attention() **\n"
          ]
        }
      ],
      "source": [
        "def verify_scaled_dot_product_attention():\n",
        "    Q = torch.arange(0, 36, dtype=torch.float).reshape([1, 6, 6])\n",
        "    K = Q\n",
        "    V = Q\n",
        "    d_k = 4\n",
        "    mask = torch.tensor([[[1, 0, 1, 0, 1, 0],\n",
        "        [0, 1, 0, 1, 0, 1],\n",
        "        [1, 0, 1, 0, 1, 0],\n",
        "        [0, 1, 0, 1, 0, 1],\n",
        "        [1, 0, 1, 0, 1, 0],\n",
        "        [0, 1, 0, 1, 0, 1]]], dtype=torch.int32)\n",
        "    output1 = scaled_dot_product_attention(Q, K, V, d_k, mask)\n",
        "    expected_output1 = torch.tensor([[[24., 25., 26., 27., 28., 29.],\n",
        "         [30., 31., 32., 33., 34., 35.],\n",
        "         [24., 25., 26., 27., 28., 29.],\n",
        "         [30., 31., 32., 33., 34., 35.],\n",
        "         [24., 25., 26., 27., 28., 29.],\n",
        "         [30., 31., 32., 33., 34., 35.]]])\n",
        "    if not torch.equal(output1, expected_output1):\n",
        "        print(f'  output tensor: {output1}')\n",
        "        print(f'expected tensor: {expected_output1}')\n",
        "        print(\"** failed (test 1, with mask) **\")\n",
        "        return\n",
        "\n",
        "    output2 = scaled_dot_product_attention(Q, K, V, d_k, None)\n",
        "    expected_output2 = torch.tensor([[[30., 31., 32., 33., 34., 35.],\n",
        "         [30., 31., 32., 33., 34., 35.],\n",
        "         [30., 31., 32., 33., 34., 35.],\n",
        "         [30., 31., 32., 33., 34., 35.],\n",
        "         [30., 31., 32., 33., 34., 35.],\n",
        "         [30., 31., 32., 33., 34., 35.]]])\n",
        "    if not torch.equal(output2, expected_output2):\n",
        "        print(f'  output tensor: {output2}')\n",
        "        print(f'expected tensor: {expected_output2}')\n",
        "        print(\"** failed (test 2, without mask) **\")\n",
        "        return\n",
        "\n",
        "    print('----------------------------------------------------------------')\n",
        "    print(\"** passed verify_scaled_dot_product_attention() **\")\n",
        "\n",
        "verify_scaled_dot_product_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f36196ab",
      "metadata": {
        "id": "f36196ab"
      },
      "source": [
        "---\n",
        "\n",
        "## [Exercise] 4.6 -- Multi-Head Attention\n",
        "\n",
        "We now have all the ingredients to implement multi-head attention. This custom PyTorch should be initialized with only two parameters -- `D` and `H`, which describes the dimension of the input embeddings, and the number of heads in the multi-head attention. See the above schematic diagram for reference, and we'll implement it as is.\n",
        "\n",
        "There are a total of 4 linear transformations -- 3 in front of `Q`, `K`, and `V`, respectively, and one after the concatenation. They are named `W_q`, `W_k`, `W_v`, and `W_o`, respectively.\n",
        "\n",
        "The `forward()` function, which takes 4 parameters, `Q`, `K`, `V`, and an optional mask,\n",
        "should be completed in the following steps:\n",
        "\n",
        "(1) We'll first take each of the three inputs (`Q`, `K`, and `V`), and perform a linear transform followed immediately by a split (using `split_heads()`).\n",
        "\n",
        "(2) Perform `scaled_dot_product_attention()` on the split versions of `Q`, `K`, and `V`.\n",
        "\n",
        "(3) combine (or concatenate) the heads using `combine_heads()`\n",
        "\n",
        "(4) linear-transform the result using `W_o`, and return the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "af234b94",
      "metadata": {
        "id": "af234b94"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, D, H):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert D % H == 0, \"D must be divisible by H\"\n",
        "        self.W_q = nn.Linear(D, D)\n",
        "        self.W_k = nn.Linear(D, D)\n",
        "        self.W_v = nn.Linear(D, D)\n",
        "        self.W_o = nn.Linear(D, D)\n",
        "        self.H = H  # Store the number of heads\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        d_k = Q.size(-1) // self.H  # Calculate d_k from the size of Q\n",
        "        Q_transformed = self.W_q(Q)\n",
        "        K_transformed = self.W_k(K)\n",
        "        V_transformed = self.W_v(V)\n",
        "\n",
        "        Q_split = split_heads(Q_transformed, self.H)\n",
        "        K_split = split_heads(K_transformed, self.H)\n",
        "        V_split = split_heads(V_transformed, self.H)\n",
        "\n",
        "        attention_output = scaled_dot_product_attention(Q_split, K_split, V_split, d_k, mask)\n",
        "\n",
        "        attention_output_combined = combine_heads(attention_output)\n",
        "        output = self.W_o(attention_output_combined)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e2d58792",
      "metadata": {
        "id": "e2d58792",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34aa1443-fbc2-4c83-cf0e-2f5adec3ab4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** passed verify_multi_head_attention() **\n"
          ]
        }
      ],
      "source": [
        "def verify_multi_head_attention():\n",
        "\n",
        "    torch.manual_seed(42)\n",
        "    multi_head_attention = MultiHeadAttention(8, 4)\n",
        "    T = torch.arange(0, 64, dtype=torch.float).reshape([1, 8, 8])\n",
        "    output = multi_head_attention(T, T, T, None)\n",
        "    expected_output = torch.tensor([[[  7.0772,   0.6164,   2.5145,   4.4008,   6.9207,  -0.1517,   0.6336,\n",
        "           11.7029],\n",
        "         [  4.9335, -20.1206,   7.0176,  -8.1752,  17.9971,  15.3381,  21.3452,\n",
        "           33.2856],\n",
        "         [  4.9166, -20.2839,   7.0531,  -8.2742,  18.0843,  15.4600,  21.5083,\n",
        "           33.4555],\n",
        "         [  4.9166, -20.2839,   7.0531,  -8.2742,  18.0843,  15.4600,  21.5083,\n",
        "           33.4555],\n",
        "         [  4.9166, -20.2839,   7.0531,  -8.2742,  18.0843,  15.4600,  21.5083,\n",
        "           33.4555],\n",
        "         [  4.9166, -20.2839,   7.0531,  -8.2742,  18.0843,  15.4600,  21.5083,\n",
        "           33.4555],\n",
        "         [  4.9166, -20.2839,   7.0531,  -8.2742,  18.0843,  15.4600,  21.5083,\n",
        "           33.4555],\n",
        "         [  4.9166, -20.2839,   7.0531,  -8.2742,  18.0843,  15.4600,  21.5083,\n",
        "           33.4555]]])\n",
        "    if output.shape != expected_output.shape or \\\n",
        "            torch.max(torch.abs(output - expected_output)) > 1e-04:\n",
        "        print(f'         output.shape = {output.shape}')\n",
        "        print(f'               output = {output}')\n",
        "        print(f'expected_output.shape = {expected_output.shape}')\n",
        "        print(f'      expected_output = {expected_output}')\n",
        "        print(\"** failed (mismatched output) **\")\n",
        "        return\n",
        "\n",
        "    print(\"** passed verify_multi_head_attention() **\")\n",
        "\n",
        "verify_multi_head_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb4d2ac1",
      "metadata": {
        "id": "bb4d2ac1"
      },
      "source": [
        "![Screenshot%202023-10-04%20at%203.59.01%20PM.png](attachment:Screenshot%202023-10-04%20at%203.59.01%20PM.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ca17ed1",
      "metadata": {
        "id": "8ca17ed1"
      },
      "source": [
        "---\n",
        "\n",
        "## [Exercise] 4.7 -- Encoder\n",
        "\n",
        "The encoder is the gray box to the left. Now we shall implement one copy of the encoder as shown in the diagram. What is slightly inaccurate is that the \"Add & Norm\" module contains an additional dropout, so it should really have been named \"Dropout & Add & Norm\". The flow is as follows:\n",
        "\n",
        "(1) The input is a `[B, L, D]` tensor that has been embedded and modified with positional encoding.\n",
        "\n",
        "(2) The same input is used as `Q`, `K`, and `V`, and fed into the multi-headed attention, without any masking. Note that the multi-headed magic happends here, and the output of the multi-headed attention has already been stitched back together. At this point, the output tensor's shape remains as `[B, L, D]`.\n",
        "\n",
        "(3) The output from the previous step goes through a dropout module, then added to the original input (see the bypass arrow), and finally layer-normalized (use `nn.LayerNorm`).\n",
        "\n",
        "(4) The output from the previous step is sent to the previously-implemented `FeedForward` module.\n",
        "\n",
        "(5) The output from the previous step goes through a dropout module, then added to the output of step (3) (see the bypass arrow), and finally layer-normalized (use `nn.LayerNorm`).\n",
        "\n",
        "Beware that we have two `nn.LayerNorm` modules but one `nn.Dropout` module. Why? Because the latter have\n",
        "no trainable parameters and hence can be shared, whereas `LayerNorms` have weights and thus must be\n",
        "treated as distinct modules.\n",
        "\n",
        "** Note: the verify_encoder() function currently only verify the shape but not the content :("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "e1e04eb9",
      "metadata": {
        "id": "e1e04eb9"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, D, H, D_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(D, H)\n",
        "        self.feed_forward = FeedForward(D, D_ff)\n",
        "        self.norm1 = nn.LayerNorm(D)\n",
        "        self.norm2 = nn.LayerNorm(D)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.self_attention(x, x, x, mask)\n",
        "        attn_output = self.dropout(attn_output)\n",
        "        x = self.norm1(x + attn_output)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        ff_output = self.dropout(ff_output)\n",
        "        x = self.norm2(x + ff_output)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "5dd8784c",
      "metadata": {
        "id": "5dd8784c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d993b7-0738-4a70-8606-3e6555475dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** passed verify_encoder() **\n"
          ]
        }
      ],
      "source": [
        "def verify_encoder():\n",
        "\n",
        "    torch.manual_seed(42)\n",
        "    encoder = EncoderLayer(8, 4, 512, 0.1)\n",
        "    source = torch.arange(0, 96, dtype=torch.float).reshape([1, 12, 8])\n",
        "    #print(source)\n",
        "    output = encoder(source)\n",
        "    #print(output)\n",
        "    expected_output = torch.tensor([[[ 0.5762, -0.6034, -1.0468,  0.4817,  0.4329, -0.9277, -0.9290,\n",
        "           2.0162],\n",
        "         [ 0.0854, -2.1007, -0.9340,  0.0277,  0.5643,  0.3290,  0.6805,\n",
        "           1.3478],\n",
        "         [ 0.3421, -1.7681, -0.8525, -0.2696,  0.6046, -0.4651,  0.7175,\n",
        "           1.6913],\n",
        "         [-0.2673, -1.2814, -1.4382, -0.3111,  0.5940,  0.2133,  0.6879,\n",
        "           1.8027],\n",
        "         [ 0.2549, -1.8962, -1.2035, -0.2756,  0.6822,  0.4724,  0.6716,\n",
        "           1.2943],\n",
        "         [ 0.2053, -1.8633, -0.5447, -0.5181,  1.1194,  0.8945,  1.2529,\n",
        "          -0.5459],\n",
        "         [-0.4650, -1.7856, -0.2822, -0.0855, -0.6335,  0.6044,  0.9789,\n",
        "           1.6684],\n",
        "         [ 0.1929, -2.1276, -0.7803, -0.3192,  0.5894,  0.4519,  0.6900,\n",
        "           1.3029],\n",
        "         [ 0.1802, -2.1396, -0.5653, -0.3975,  0.2714,  0.4724,  0.7409,\n",
        "           1.4375],\n",
        "         [ 0.2608, -1.8195, -0.5088, -0.4074,  0.7563,  0.6471, -0.6113,\n",
        "           1.6829],\n",
        "         [-0.0986, -2.1989, -0.4897, -0.2996,  0.6097,  0.4313,  0.7269,\n",
        "           1.3189],\n",
        "         [ 0.2976, -1.7122, -0.6534, -1.2018,  0.6852,  0.5508,  0.6512,\n",
        "           1.3825]]])\n",
        "    if output.shape != expected_output.shape or \\\n",
        "            False: # FIXME torch.max(torch.abs(output - expected_output)) > 1e-04:\n",
        "        print(f'         output.shape = {output.shape}')\n",
        "        print(f'               output = {output}')\n",
        "        print(f'expected_output.shape = {expected_output.shape}')\n",
        "        print(f'      expected_output = {expected_output}')\n",
        "        print(\"** failed (mismatched output) **\")\n",
        "        return\n",
        "\n",
        "    print(\"** passed verify_encoder() **\")\n",
        "\n",
        "verify_encoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f92cb6",
      "metadata": {
        "id": "f9f92cb6"
      },
      "source": [
        "---\n",
        "\n",
        "## [Exercise] 4.8 -- Decoder\n",
        "\n",
        "This is almost identical to the encoder, but slightly more complicated.\n",
        "\n",
        "(1) The input is a `[B, L, D]` tensor that has been embedded and modified with positional encoding.\n",
        "\n",
        "(2) The same input is used as `Q`, `K`, and `V`, and fed into the multi-headed attention. This time, you\n",
        "need to use a causality-mask.\n",
        "\n",
        "(3) The output from the previous step goes through a dropout module, then added to the original input (see the bypass arrow), and finally layer-normalized (use `nn.LayerNorm`).\n",
        "\n",
        "(4) The output from the previous step got fed into the cross attention as the query `Q`, whereas\n",
        "`encoder_output` (which was passed in) is used as both `K` and `V` (see the diagram). Cross attention\n",
        "does not need any masking.\n",
        "\n",
        "(5) Another round of dropout, addition, and layer normalization.\n",
        "\n",
        "(6) The output from the previous step is sent to the previously-implemented FeedForward module.\n",
        "\n",
        "(7) Another round of dropout, addition, and layer normalization.\n",
        "\n",
        "Hopefully you are getting the gist of this process?\n",
        "\n",
        "** Note: the verify_decoder() function currently only verify the shape but not the content :("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "cf45f2e2",
      "metadata": {
        "id": "cf45f2e2"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, D, H, D_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(D, H)\n",
        "        self.cross_attention = MultiHeadAttention(D, H)\n",
        "        self.feed_forward = FeedForward(D, D_ff)\n",
        "        self.norm1 = nn.LayerNorm(D)\n",
        "        self.norm2 = nn.LayerNorm(D)\n",
        "        self.norm3 = nn.LayerNorm(D)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, self_mask, cross_mask=None):\n",
        "        self_attention_output = self.self_attention(x, x, x, self_mask)\n",
        "        self_attention_output = self.dropout(self_attention_output)\n",
        "        x = self.norm1(x + self_attention_output)\n",
        "\n",
        "        cross_attention_output = self.cross_attention(x, encoder_output, encoder_output, cross_mask)\n",
        "        cross_attention_output = self.dropout(cross_attention_output)\n",
        "        x = self.norm2(x + cross_attention_output)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        ff_output = self.dropout(ff_output)\n",
        "        x = self.norm3(x + ff_output)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "e7090a94",
      "metadata": {
        "id": "e7090a94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec267f50-fa92-43cd-aaa9-e54fea655d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** passed verify_decoder() **\n"
          ]
        }
      ],
      "source": [
        "def verify_decoder():\n",
        "\n",
        "    torch.manual_seed(42)\n",
        "    encoder = EncoderLayer(8, 4, 512, 0.1)\n",
        "    decoder = DecoderLayer(8, 4, 512, 0.1)\n",
        "\n",
        "    source = torch.arange(100, 196, dtype=torch.float).reshape([1, 12, 8])\n",
        "    target = torch.arange(0, 96, dtype=torch.float).reshape([1, 12, 8])\n",
        "\n",
        "    encoder_output = encoder(source)\n",
        "    causality_mask = generate_causality_mask(target)\n",
        "    output = decoder(target, encoder_output, causality_mask)\n",
        "    expected_output = torch.tensor([[[-1.2194, -0.8877, -0.8026,  0.6160, -0.9453,  0.7488,  1.5156,\n",
        "           0.9746],\n",
        "         [-0.1498, -1.0341, -1.4889,  0.0059, -0.7573,  1.3584,  0.7692,\n",
        "           1.2964],\n",
        "         [-1.5095, -0.4470, -1.2332,  0.2313, -0.1101,  1.7068,  0.4629,\n",
        "           0.8987],\n",
        "         [ 0.9942, -0.6095, -1.8409, -0.3147,  0.4359,  1.4783, -0.6907,\n",
        "           0.5473],\n",
        "         [ 0.9621, -0.3903, -1.8374, -0.5268,  0.5258,  1.1402, -0.8544,\n",
        "           0.9808],\n",
        "         [ 1.1269, -0.5522, -1.7475, -0.4149,  0.9871,  1.1282, -0.9014,\n",
        "           0.3738],\n",
        "         [ 1.3428, -0.0837, -1.8869, -0.3617,  0.7708,  1.1620, -0.7054,\n",
        "          -0.2379],\n",
        "         [ 1.7143,  0.1166, -1.6716, -0.0446, -0.5684,  1.2267, -0.6361,\n",
        "          -0.1368],\n",
        "         [ 1.5492, -0.7092, -0.9928, -0.4902,  1.0260,  1.2030, -1.0311,\n",
        "          -0.5548],\n",
        "         [ 1.4449,  0.1218, -1.6833, -0.5653,  0.9548,  0.9817, -0.8295,\n",
        "          -0.4251],\n",
        "         [ 1.0121, -0.7248, -2.0945, -0.5780,  0.9053,  0.7150,  0.6125,\n",
        "           0.1523],\n",
        "         [ 1.4806,  0.1730, -1.6805, -0.6377,  1.0146,  0.8628, -0.7439,\n",
        "          -0.4688]]])\n",
        "    if output.shape != expected_output.shape or \\\n",
        "            False: # FIXME torch.max(torch.abs(output - expected_output)) > 1e-04:\n",
        "        print(f'         output.shape = {output.shape}')\n",
        "        print(f'               output = {output}')\n",
        "        print(f'expected_output.shape = {expected_output.shape}')\n",
        "        print(f'      expected_output = {expected_output}')\n",
        "        print(\"** failed (mismatched output) **\")\n",
        "        return\n",
        "\n",
        "    print(\"** passed verify_decoder() **\")\n",
        "\n",
        "verify_decoder()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def verify_decoder():\n",
        "\n",
        "#     torch.manual_seed(42)\n",
        "#     encoder = EncoderLayer(8, 4, 512, 0.1)\n",
        "#     decoder = DecoderLayer(8, 4, 512, 0.1)\n",
        "\n",
        "#     source = torch.arange(100, 196, dtype=torch.float).reshape([1, 12, 8])\n",
        "#     target = torch.arange(0, 96, dtype=torch.float).reshape([1, 12, 8])\n",
        "\n",
        "#     encoder_output = encoder(source)\n",
        "#     causality_mask = generate_causality_mask(target)\n",
        "#     output = decoder(target, encoder_output, causality_mask)\n",
        "#     expected_output = torch.tensor([[[-1.2194, -0.8877, -0.8026,  0.6160, -0.9453,  0.7488,  1.5156,\n",
        "#            0.9746],\n",
        "#          [-0.1498, -1.0341, -1.4889,  0.0059, -0.7573,  1.3584,  0.7692,\n",
        "#            1.2964],\n",
        "#          [-1.5095, -0.4470, -1.2332,  0.2313, -0.1101,  1.7068,  0.4629,\n",
        "#            0.8987],\n",
        "#          [ 0.9942, -0.6095, -1.8409, -0.3147,  0.4359,  1.4783, -0.6907,\n",
        "#            0.5473],\n",
        "#          [ 0.9621, -0.3903, -1.8374, -0.5268,  0.5258,  1.1402, -0.8544,\n",
        "#            0.9808],\n",
        "#          [ 1.1269, -0.5522, -1.7475, -0.4149,  0.9871,  1.1282, -0.9014,\n",
        "#            0.3738],\n",
        "#          [ 1.3428, -0.0837, -1.8869, -0.3617,  0.7708,  1.1620, -0.7054,\n",
        "#           -0.2379],\n",
        "#          [ 1.7143,  0.1166, -1.6716, -0.0446, -0.5684,  1.2267, -0.6361,\n",
        "#           -0.1368],\n",
        "#          [ 1.5492, -0.7092, -0.9928, -0.4902,  1.0260,  1.2030, -1.0311,\n",
        "#           -0.5548],\n",
        "#          [ 1.4449,  0.1218, -1.6833, -0.5653,  0.9548,  0.9817, -0.8295,\n",
        "#           -0.4251],\n",
        "#          [ 1.0121, -0.7248, -2.0945, -0.5780,  0.9053,  0.7150,  0.6125,\n",
        "#            0.1523],\n",
        "#          [ 1.4806,  0.1730, -1.6805, -0.6377,  1.0146,  0.8628, -0.7439,\n",
        "#           -0.4688]]])\n",
        "#     if output.shape != expected_output.shape or \\\n",
        "#         torch.max(torch.abs(output - expected_output)) > 1e-04:\n",
        "#         print(f'         output.shape = {output.shape}')\n",
        "#         print(f'               output = {output}')\n",
        "#         print(f'expected_output.shape = {expected_output.shape}')\n",
        "#         print(f'      expected_output = {expected_output}')\n",
        "#         print(\"** failed (mismatched output) **\")\n",
        "#         return\n",
        "\n",
        "#     print(\"** passed verify_decoder() **\")\n",
        "\n",
        "# verify_decoder()"
      ],
      "metadata": {
        "id": "Isc0oXxIJ6SM"
      },
      "id": "Isc0oXxIJ6SM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dacb00e1",
      "metadata": {
        "id": "dacb00e1"
      },
      "source": [
        "![Screenshot%202023-10-04%20at%203.59.01%20PM.png](attachment:Screenshot%202023-10-04%20at%203.59.01%20PM.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a03a95a",
      "metadata": {
        "id": "3a03a95a"
      },
      "source": [
        "---\n",
        "\n",
        "## [Exercise] 4.9 -- Transformer\n",
        "\n",
        "We are finally ready to put everything together, using the modules we have implement earlier. By now,\n",
        "hopefully, it would not be difficult to do it.\n",
        "\n",
        "---\n",
        "\n",
        "The following are instructions for the constructor:\n",
        "\n",
        "(1) First, the transformer need to be initialized with the following parameters:\n",
        "\n",
        "```\n",
        "    source_vocab_size\n",
        "    target_vocab_size\n",
        "    D -- dimension of the embedding\n",
        "    H -- number of heads\n",
        "    Nx -- number of stacked encoders and decoders (see diagram)\n",
        "    D_ff -- output dimension of the feed-forward\n",
        "    maxL -- maximum sequence length\n",
        "    dropout -- dropout factor\n",
        "```   \n",
        "\n",
        "  You also need to instantiate the modules used accordingly. For your convenience, I have listed them\n",
        "  in the `__init__()` function, but you do need to properly initialize them.\n",
        "  \n",
        "  \n",
        "(2) `encoder_embedding` should be initialized with `source_vocab_size` as the first parameter, and`decoder_embedding` should be initialized with `target_vocab_size` as the first parameter.\n",
        "\n",
        "(3) Note that the number of `encoder_layers` and `decoder_layers` are determined by the `Nx`\n",
        "parameter. As such, we cannot hard-code the layers but instead must use lists. However,\n",
        "as mentioned in the tutorial, you need to use `nn.ModuleList` instead of just Python list so that\n",
        "PyTorch can see them as optimizable. To be clear, you will make `Nx` copies of the encoder and\n",
        "`Nx` copies of the decoder -- i.e. you cannot share the module, as the weights of each encoder\n",
        "(and decoder) are uniquely and separately optimized.\n",
        "\n",
        "(4) The input to the transformer `forward()` function is simply `source` and `target`. You\n",
        "should know by now that both are tensors of shape `[B, L]`, where `L <= maxL`. Note that\n",
        "the parameter `maxL` is used in the constructor of the positional encoder, but you're allowed\n",
        "to pass in tensors with the length dimension smaller than `maxL`.\n",
        "\n",
        "(5) Now look at the diagram carefully. Make sure you initialize each of the module properly\n",
        "with the right arguments. Note that the parameters `H` (number of heads) and `D_ff` (output\n",
        "dimension of the feedforward) are needed by both the encoder layers and the decoder layers.\n",
        "\n",
        "(6) The role of the `fc` module, which is a fully-connected `nn.Linear` module\n",
        "after the encoder stack, is to project the embeddings (of dimension `D`)\n",
        "to the target vocabulary (of dimension `target_vocab_size`). In short, for each target token,\n",
        "we convert the target embedding of the corresponding token\n",
        "to some kind of unweighted scores.\n",
        "\n",
        "---\n",
        "\n",
        "The following are instructions for the `forward()` function.\n",
        "\n",
        "(7) `nn.Dropout` is a module that is used at two places (see below). However, since it is parameter-less,\n",
        "you can use it at multiple places without any repercussion. Furthermore,\n",
        "`positional_encoding` is also parameter-less and therefore can be shared.\n",
        "\n",
        "(7) On the encoder side, you need to take the source tensor, apply the source embedding,\n",
        "    apply the positional encoding, and apply dropout (this is needed and described in the paper\n",
        "    but not explicitly shown in the diagram). Let the output be called `encoder_output`.\n",
        "    \n",
        "(8) On the decoder side, you need to take the target tensor, apply the target embedding,\n",
        "    apply the positional encoding, and apply dropout (this is needed and described in the paper\n",
        "    but not explicitly shown in the diagram). Let the output be called `decoder_output`.\n",
        "    \n",
        "(9) Call `generate_causality_mask(target)` to generate the causality mask.\n",
        "\n",
        "(10) Take `encoder_output` and feed it through the stack of `Nx` encoders, each call of\n",
        "which not only take the previous `encoder_output`. Let\n",
        "the final encode output be called `C` (the source context, see Part III).\n",
        "\n",
        "(11) Take `decoder_output` and feed it through the stack of `Nx` decoders, each call of\n",
        "which not only take the previous `decoder_output`, but also the causality maak,\n",
        "as well as `C`, which is the very last encoder output.\n",
        "\n",
        "(12) Take `decoder_output` and apply `fc` to get the unweighted likelihood measurement.\n",
        "\n",
        "(13) That's it! There is no need for the `softmax` here (we'll explain why later).\n",
        "\n",
        "---\n",
        "\n",
        "At first glance, this may look overwhelming, but we have just describe how to translate the diagram\n",
        "to actual PyTorch code. The diagram actually did a very good job conveying all the details\n",
        "necessarily to implement the transformer module, with only two minor details:\n",
        "\n",
        "(a) The use of `nn.Dropout` after positional encoding and before encoding or decoding.\n",
        "\n",
        "(b) The omission of the `softmax` module, which is actually not a bug but a feature --\n",
        "the `softmax` is calculated as part of the loss function `CrossEntropyLoss()`.\n",
        "\n",
        "** Note: the verify_transformer() function currently only verify the shape but not the content :("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "002e78f7",
      "metadata": {
        "id": "002e78f7"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, source_vocab_size, target_vocab_size, D, H, Nx, D_ff, maxL, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(source_vocab_size, D)\n",
        "        self.decoder_embedding = nn.Embedding(target_vocab_size, D)\n",
        "        self.positional_encoding = PositionalEncoding(D, maxL)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(D, H, D_ff, dropout) for _ in range(Nx)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(D, H, D_ff, dropout) for _ in range(Nx)])\n",
        "\n",
        "        self.fc = nn.Linear(D, target_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        source_embedded = self.encoder_embedding(source)\n",
        "        source_encoded = self.positional_encoding(source_embedded)\n",
        "        encoder_output = self.dropout(source_encoded)\n",
        "\n",
        "        target_embedded = self.decoder_embedding(target)\n",
        "        target_encoded = self.positional_encoding(target_embedded)\n",
        "        decoder_output = self.dropout(target_encoded)\n",
        "\n",
        "        causality_mask = generate_causality_mask(target)\n",
        "\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            encoder_output = encoder_layer(encoder_output, None)\n",
        "\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            decoder_output = decoder_layer(decoder_output, encoder_output, causality_mask, None)\n",
        "\n",
        "        output = self.fc(decoder_output)\n",
        "        return output\n",
        "\n",
        "# Assuming the necessary helper functions and classes like PositionalEncoding, EncoderLayer,\n",
        "# DecoderLayer, and generate_causality_mask are defined elsewhere in the code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "51bf4655",
      "metadata": {
        "id": "51bf4655",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf47ba68-a989-4747-a38d-36cfaf116df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** passed verify_transformer() **\n"
          ]
        }
      ],
      "source": [
        "def verify_transformer():\n",
        "\n",
        "    torch.manual_seed(42)\n",
        "    source_vocab_size = 10\n",
        "    target_vocab_size = 10\n",
        "    D = 8\n",
        "    H = 2\n",
        "    Nx = 6\n",
        "    D_ff = 20\n",
        "    maxL = 100\n",
        "    dropout = 0.1\n",
        "    transformer = Transformer(source_vocab_size, target_vocab_size,\n",
        "                              D, H, Nx, D_ff, maxL, dropout)\n",
        "    B = 2\n",
        "    Ls = 5\n",
        "    Lt = 5\n",
        "    source = torch.randint(1, source_vocab_size, (B, Ls))\n",
        "    target = torch.randint(1, source_vocab_size, (B, Lt))\n",
        "    output = transformer(source, target)\n",
        "    expected_output = torch.tensor([[[ 1.2637, -1.2294,  0.2417,  0.1209, -0.3542,  0.4997, -0.8134,\n",
        "           0.0673, -0.0027,  0.0205],\n",
        "         [ 0.7656, -0.0395, -0.3658, -0.2799, -1.2464,  0.4382, -0.3345,\n",
        "          -0.7046,  0.2030,  0.2935],\n",
        "         [ 0.9448, -1.0114,  0.5522, -0.4954, -0.7464,  0.1261, -0.4729,\n",
        "          -0.0512,  0.2308, -0.3556],\n",
        "         [ 1.3371, -0.7485, -0.0671, -0.0582, -1.0672,  0.6028, -0.7285,\n",
        "          -0.4926,  0.0692,  0.1587],\n",
        "         [ 1.1082, -0.3944, -0.6571,  0.3137, -0.8469,  0.9551, -0.8199,\n",
        "          -0.2551,  0.1223,  0.7416]],\n",
        "\n",
        "        [[ 0.6412,  0.5002, -0.2971, -0.4550, -1.1884,  0.7841, -0.5566,\n",
        "          -0.6515,  0.1819,  0.2816],\n",
        "         [ 1.0129, -0.9239,  0.3055, -0.2479, -0.8097,  0.3372, -1.0252,\n",
        "          -0.4146,  0.0477, -0.0251],\n",
        "         [ 1.2067, -0.5949, -0.4990,  0.2367, -0.9443,  0.8509, -1.0912,\n",
        "          -0.4610,  0.0049,  0.6044],\n",
        "         [ 0.9175, -0.6736, -0.5932,  0.2163, -0.8875,  0.6833, -1.1990,\n",
        "          -0.3715,  0.0667,  0.7133],\n",
        "         [ 1.4838, -0.4566, -0.4166,  0.3582, -0.7404,  1.0352, -0.9805,\n",
        "          -0.5391, -0.2400,  0.4358]]])\n",
        "    if output.shape != expected_output.shape or \\\n",
        "            False: # FIXME torch.max(torch.abs(output - expected_output)) > 1e-04:\n",
        "        print(f'         output.shape = {output.shape}')\n",
        "        print(f'               output = {output}')\n",
        "        print(f'expected_output.shape = {expected_output.shape}')\n",
        "        print(f'      expected_output = {expected_output}')\n",
        "        print(\"** failed (mismatched output) **\")\n",
        "        return\n",
        "\n",
        "    print(\"** passed verify_transformer() **\")\n",
        "\n",
        "verify_transformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f21ab0c3",
      "metadata": {
        "id": "f21ab0c3"
      },
      "source": [
        "---\n",
        "\n",
        "## [Exercise] 4.10 -- Training\n",
        "\n",
        "Congratulations! We have completed the implementation of transformer. That's all the code you need to write. The following is some training code to test the transformer. No need to code anything below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "e84b1ada",
      "metadata": {
        "scrolled": false,
        "id": "e84b1ada"
      },
      "outputs": [],
      "source": [
        "source_vocab_size = 5000\n",
        "target_vocab_size = 5000\n",
        "D = 48\n",
        "H = 3\n",
        "Nx = 3\n",
        "D_ff = 128\n",
        "maxL = 20\n",
        "dropout = 0.1\n",
        "\n",
        "transformer = Transformer(source_vocab_size, target_vocab_size,\n",
        "                         D, H, Nx, D_ff, maxL, dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "24218d18",
      "metadata": {
        "id": "24218d18"
      },
      "outputs": [],
      "source": [
        "B = 64\n",
        "Ls = maxL  # arbitrary, but must be > 0 and <= maxL\n",
        "Lt = maxL  # arbitrary, but must be > 0 and <= maxL\n",
        "source_data = torch.randint(1, source_vocab_size, (B, Ls))\n",
        "target_data = torch.randint(1, target_vocab_size, (B, Lt))\n",
        "\n",
        "# generate a [B, L] tensor whose values are [1, source_vocab_size)\n",
        "# generate a [B, L] tensor whose values are [1, target_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "6b864b5f",
      "metadata": {
        "scrolled": true,
        "id": "6b864b5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af94bc1-fa65-4dbc-e051-55bebfddeaf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/5001 [00:00<25:05,  3.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 8.701016426086426)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 52/5001 [00:10<13:59,  5.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 51, Loss: 8.300421714782715)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 102/5001 [00:19<14:21,  5.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 101, Loss: 7.934391021728516)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 151/5001 [00:28<18:42,  4.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 151, Loss: 7.5954484939575195)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 202/5001 [00:37<13:56,  5.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 201, Loss: 7.270991325378418)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 252/5001 [00:47<14:18,  5.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 251, Loss: 6.982875823974609)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 302/5001 [00:56<14:33,  5.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 301, Loss: 6.686288833618164)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 352/5001 [01:05<13:31,  5.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 351, Loss: 6.421304702758789)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 402/5001 [01:15<14:10,  5.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 401, Loss: 6.166356086730957)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▉         | 452/5001 [01:25<13:20,  5.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 451, Loss: 5.911505222320557)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 502/5001 [01:34<15:41,  4.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 501, Loss: 5.685304164886475)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 552/5001 [01:43<13:05,  5.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 551, Loss: 5.453693389892578)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 602/5001 [01:53<12:34,  5.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 601, Loss: 5.215754985809326)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 652/5001 [02:02<12:32,  5.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 651, Loss: 4.99837589263916)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 701/5001 [02:11<17:02,  4.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 701, Loss: 4.761020660400391)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 752/5001 [02:20<12:48,  5.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 751, Loss: 4.5394062995910645)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 802/5001 [02:30<12:28,  5.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 801, Loss: 4.333608627319336)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 852/5001 [02:40<12:55,  5.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 851, Loss: 4.113043785095215)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 902/5001 [02:48<11:51,  5.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 901, Loss: 3.908545732498169)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 952/5001 [02:58<11:41,  5.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 951, Loss: 3.6963589191436768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1002/5001 [03:07<11:43,  5.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1001, Loss: 3.4988842010498047)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 1051/5001 [03:17<17:22,  3.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1051, Loss: 3.3025906085968018)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 1102/5001 [03:26<11:28,  5.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1101, Loss: 3.1028411388397217)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 1152/5001 [03:36<11:29,  5.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1151, Loss: 2.9010281562805176)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 1202/5001 [03:45<11:20,  5.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1201, Loss: 2.7325589656829834)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1251/5001 [03:54<15:50,  3.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1251, Loss: 2.5408222675323486)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 1302/5001 [04:04<10:21,  5.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1301, Loss: 2.3474109172821045)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 1352/5001 [04:13<10:31,  5.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1351, Loss: 2.198892593383789)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 1402/5001 [04:23<10:53,  5.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1401, Loss: 2.0444130897521973)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 1451/5001 [04:31<10:37,  5.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1451, Loss: 1.863722801208496)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 1502/5001 [04:41<10:01,  5.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1501, Loss: 1.7128422260284424)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 1552/5001 [04:51<10:25,  5.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1551, Loss: 1.554205298423767)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 1602/5001 [05:00<11:30,  4.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1601, Loss: 1.4356403350830078)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 1652/5001 [05:09<10:07,  5.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1651, Loss: 1.3069711923599243)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 1702/5001 [05:19<09:42,  5.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1701, Loss: 1.1898760795593262)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 1752/5001 [05:29<09:09,  5.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1751, Loss: 1.0698055028915405)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 1801/5001 [05:38<13:23,  3.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1801, Loss: 0.9652607440948486)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 1852/5001 [05:47<09:25,  5.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1851, Loss: 0.8737204670906067)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 1902/5001 [05:56<08:42,  5.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1901, Loss: 0.771584153175354)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 1952/5001 [06:06<09:13,  5.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1951, Loss: 0.6809811592102051)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 2001/5001 [06:15<10:58,  4.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2001, Loss: 0.600385308265686)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 2051/5001 [06:24<08:38,  5.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2051, Loss: 0.5358302593231201)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 2102/5001 [06:34<08:34,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2101, Loss: 0.47189778089523315)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 2152/5001 [06:44<09:08,  5.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2151, Loss: 0.42659372091293335)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 2202/5001 [06:52<08:10,  5.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2201, Loss: 0.37905633449554443)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 2252/5001 [07:02<07:56,  5.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2251, Loss: 0.33287692070007324)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 2302/5001 [07:12<07:53,  5.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2301, Loss: 0.2874150276184082)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 2351/5001 [07:21<10:46,  4.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2351, Loss: 0.25871339440345764)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 2402/5001 [07:30<07:55,  5.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2401, Loss: 0.22621594369411469)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|████▉     | 2452/5001 [07:40<07:39,  5.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2451, Loss: 0.19678883254528046)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 2502/5001 [07:49<07:23,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2501, Loss: 0.17227225005626678)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 2551/5001 [07:58<08:53,  4.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2551, Loss: 0.16015876829624176)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 2602/5001 [08:07<06:45,  5.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2601, Loss: 0.1377360224723816)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|█████▎    | 2652/5001 [08:17<07:07,  5.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2651, Loss: 0.12511496245861053)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 2702/5001 [08:27<07:13,  5.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2701, Loss: 0.10852664709091187)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 2752/5001 [08:36<06:38,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2751, Loss: 0.10000347346067429)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 2802/5001 [08:45<06:35,  5.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2801, Loss: 0.08575228601694107)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 2852/5001 [08:55<06:24,  5.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2851, Loss: 0.0773586630821228)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 2902/5001 [09:05<07:02,  4.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2901, Loss: 0.06688863039016724)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 2952/5001 [09:14<06:08,  5.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2951, Loss: 0.06812397390604019)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 3002/5001 [09:23<05:53,  5.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3001, Loss: 0.05500917509198189)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 3052/5001 [09:33<05:56,  5.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3051, Loss: 0.0497191958129406)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 3101/5001 [09:42<08:07,  3.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3101, Loss: 0.04301251098513603)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|██████▎   | 3152/5001 [09:51<05:21,  5.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3151, Loss: 0.036239031702280045)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 3202/5001 [10:01<05:34,  5.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3201, Loss: 0.03690912202000618)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 3252/5001 [10:11<05:08,  5.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3251, Loss: 0.030615292489528656)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 66%|██████▌   | 3301/5001 [10:20<06:41,  4.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3301, Loss: 0.02769920974969864)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 3352/5001 [10:29<04:49,  5.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3351, Loss: 0.0254951361566782)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 3402/5001 [10:39<04:40,  5.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3401, Loss: 0.022276120260357857)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▉   | 3452/5001 [10:48<04:58,  5.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3451, Loss: 0.021195290610194206)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 3502/5001 [10:57<04:31,  5.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3501, Loss: 0.018594427034258842)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 3552/5001 [11:07<04:24,  5.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3551, Loss: 0.015696663409471512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 3602/5001 [11:16<04:02,  5.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3601, Loss: 0.014517809264361858)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 3652/5001 [11:26<05:14,  4.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3651, Loss: 0.014058135449886322)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▍  | 3702/5001 [11:35<03:50,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3701, Loss: 0.011994459666311741)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 3752/5001 [11:44<03:42,  5.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3751, Loss: 0.010850239545106888)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▌  | 3802/5001 [11:54<03:35,  5.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3801, Loss: 0.010039938613772392)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 3851/5001 [12:03<04:45,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3851, Loss: 0.008469698019325733)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 3902/5001 [12:12<03:06,  5.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3901, Loss: 0.008513889275491238)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 79%|███████▉  | 3952/5001 [12:22<03:07,  5.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3951, Loss: 0.007604081183671951)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 4001/5001 [12:32<03:06,  5.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4001, Loss: 0.006837798748165369)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 4051/5001 [12:40<02:56,  5.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4051, Loss: 0.005379877984523773)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 4102/5001 [12:50<02:31,  5.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4101, Loss: 0.005364008713513613)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 4152/5001 [13:00<02:33,  5.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4151, Loss: 0.005303532816469669)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▍ | 4202/5001 [13:09<03:08,  4.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4201, Loss: 0.004807972349226475)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▌ | 4252/5001 [13:18<02:10,  5.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4251, Loss: 0.004126607906073332)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▌ | 4302/5001 [13:28<02:04,  5.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4301, Loss: 0.0038688096683472395)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 4352/5001 [13:37<01:52,  5.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4351, Loss: 0.003411059733480215)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 4401/5001 [13:46<02:26,  4.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4401, Loss: 0.0031312431674450636)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|████████▉ | 4452/5001 [13:56<01:38,  5.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4451, Loss: 0.0028422169853001833)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 4502/5001 [14:05<01:31,  5.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4501, Loss: 0.002675405703485012)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|█████████ | 4552/5001 [14:15<01:20,  5.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4551, Loss: 0.0022609811276197433)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 4601/5001 [14:24<01:22,  4.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4601, Loss: 0.002214941428974271)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|█████████▎| 4652/5001 [14:34<00:59,  5.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4651, Loss: 0.0022607885766774416)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 4702/5001 [14:43<00:52,  5.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4701, Loss: 0.001826804713346064)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 4752/5001 [14:53<00:47,  5.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4751, Loss: 0.001686137169599533)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 4802/5001 [15:02<00:35,  5.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4801, Loss: 0.0016360062872990966)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 4852/5001 [15:11<00:25,  5.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4851, Loss: 0.0014168298803269863)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 4901/5001 [15:21<00:17,  5.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4901, Loss: 0.0012215982424095273)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 4951/5001 [15:30<00:12,  3.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4951, Loss: 0.0013329264475032687)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5001/5001 [15:39<00:00,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5001, Loss: 0.0010944067034870386)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    # Hardy: basically, CrossEntropyLoss() compares the output tensor to\n",
        "    # the target. Conceptually, say we are to compare a K-dimensional\n",
        "    # vector to an index, where K corresponds to the size of the vocab,\n",
        "    # which we'll do a softmax. Then, we'll simply take the value of the\n",
        "    # softmax at the given index, and take a negative log. We do that to\n",
        "    # all outputs, and take the average. That's the cross entropy loss.\n",
        "    #\n",
        "    # For example, let's say K = 4, and the tensor is\n",
        "    # tensor([0.5, 0.6, 0.7, 0.8]). Note that they don't need to sum to\n",
        "    # one. Then softmax gives tensor([0.2138, 0.2363, 0.2612, 0.2887]),\n",
        "    # which sum to one. Let the target index be 2, so the cross entropy\n",
        "    # is -np.log(0.2612) = 1.342468881579861. The higher the confidence,\n",
        "    # the lower the cross entropy. These are some reference numbers:\n",
        "    #    -np.log(0.1) = 2.30\n",
        "    #    -np.log(0.5) = 0.69\n",
        "    #    -np.log(0.8) = 0.22\n",
        "    #    -np.log(0.9) = 0.11\n",
        "    #    -np.log(0.95) = 0.05\n",
        "    #    -np.log(0.99) = 0.01\n",
        "\n",
        "optimizer = optim.Adam(transformer.parameters(),\n",
        "                      lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
        "    # Hardy: these are hyperparameters used in the Attention paper\n",
        "\n",
        "transformer.train()\n",
        "for epoch in tqdm(range(5001)):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(source_data, target_data[:, :-1])\n",
        "    loss = criterion(output.reshape(\n",
        "        -1, target_vocab_size), target_data[:, 1:].reshape(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 50 == 0:\n",
        "        tqdm.write(f'Epoch: {epoch+1}, Loss: {loss.item()})')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KxEqMXh_KiXT"
      },
      "id": "KxEqMXh_KiXT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}