{"id": "1", "contributed_by": "group 3", "question": "What is a Hyperplane", "answers": ["A hyperplane is a flat affine subspace of dimension p âˆ’ 1 in a p dimensional plane"]}
{"id": "2", "contributed_by": "group 3", "question": "When can we use maximal margin classifier as a natural way to perform classification", "answers": ["The maximal margin classifier is a very natural way to perform classification if a separating hyperplane exists"]}
{"id": "3", "contributed_by": "group 3", "question": "Which classifier is a soft margin classifier", "answers": ["The support vector classifier can also be called a soft margin classifier"]}
{"id": "4", "contributed_by": "group 3", "question": "What is a support vector machine", "answers": ["The support vector machine is an extension of the support vector classifier that results from enlarging the feature space in a specific way using kernels"]}
{"id": "5", "contributed_by": "group 3", "question": "What are popular approaches to use SVM for classifying more than two classes", "answers": ["The two approaches are one versus one classification and one vs all classification"]}
{"id": "6", "contributed_by": "group 3", "question": "Which loss function is similar to the one used in logistic regression", "answers": ["Hinge loss function is closely related to the loss function used in logistic regression"]}
{"id": "7", "contributed_by": "group 3", "question": "What is an extension of the SVM for regression", "answers": ["The extension of the SVM for regression is called support vector regression"]}
{"id": "8", "contributed_by": "group 3", "question": "Why is the margin soft in soft margin classifier", "answers": ["The margin is soft because it can be violated by some of the training observations"]}
{"id": "9", "contributed_by": "group 3", "question": "What are support vectors", "answers": ["Observations that lie directly on the margin, or on the wrong side of the margin for their class, are known as support vectors"]}
{"id": "10", "contributed_by": "group 3", "question": "What is generalization of the maximal margin classifier to the non-separable case known as", "answers": ["The generalization of the maximal margin classifier to the non-separable case is known as the support vector classifier."]}
{"id": "11", "contributed_by": "group 3", "question": "What variables allow individual observations to be on the wrong side of the margin or the hyperplane", "answers": ["slack variables allow individual observations to be on the wrong side of the margin or the hyperplane"]}
{"id": "12", "contributed_by": "group 3", "question": "How can we address the problem of non linear boundaries in support vector classifier", "answers": ["we can address the problem of non-linear boundaries enlarging the feature space using functions of the feature space"]}
{"id": "13", "contributed_by": "group 3", "question": "Why is support vector classifer robust to the behavior of observations that are far away from the hyperplane", "answers": ["support vector classifiers decision rule is based only on a potentially small subset of the training observations"]}
{"id": "14", "contributed_by": "group 3", "question": "What can be used to enlarge the feature space used by the support vector classifier in a way that leads to efficient computations", "answers": ["Support vector machine allows us to enlarge the feature space used by the support vector classifier in a way that leads to efficient computations"]}
{"id": "15", "contributed_by": "group 3", "question": "What is a kernel", "answers": ["A kernel is a function that quantifies the similarity of two observations"]}
{"id": "16", "contributed_by": "group 3", "question": "What does linear kernel do", "answers": ["linear kernel quantifies the similarity of a pair of observations using Pearson correlation"]}
{"id": "17", "contributed_by": "group 3", "question": "what classifier is the result of combining support vector classifier with non linear kernel space", "answers": ["The resulting classifier is known as support vector machine"]}
{"id": "18", "contributed_by": "group 3", "question": "When do SVMs tend to behave better than logistic regression", "answers": ["When the classes are well separated"]}
{"id": "19", "contributed_by": "group 3", "question": "What happens when p is large for maximal margin classifier", "answers": ["It can lead to overfitting"]}
{"id": "20", "contributed_by": "group 3", "question": "What is an optimal separating hyperplane", "answers": ["Separating hyperplane that is farthest from the training observations"]}
{"id": "21", "contributed_by": "group 3", "question": "What is controlled by the tuning parameter C", "answers": ["C controls the bias-variance trade-off of the statistical learning technique."]}
{"id": "22", "contributed_by": "group 3", "question": "what function can be used to to produce several ROC plots", "answers": ["RocCurveDisplay.from_estimator()"]}
{"id": "23", "contributed_by": "group 3", "question": "What happens if we use a smaller value of the cost parameter", "answers": ["we obtain a large number of support vectors"]}
{"id": "24", "contributed_by": "group 3", "question": "What are the outputs for SVMs and support vector classifiers", "answers": ["SVMs and support vector classifiers output class labels for each observation"]}
{"id": "25", "contributed_by": "group 3", "question": "What function can be used to visualize support vector classifier with two features", "answers": ["Support vector classifier with two features can be visualized by plotting values of its decision function"]}

{"id": "30", "contributed_by": "group 3", "question": "What are tree-based methods used for?", "answers": ["Tree-based methods are used for regression and classification."]},
{"id": "31", "contributed_by": "group 3", "question": "What are the two main steps for predicting using a regression tree?", "answers": ["1. Divide the predictor space into distinct non-overlapping regions. 2. For each region, make a prediction using the mean response of training observations in that region."]},
{"id": "32", "contributed_by": "group 3", "question": "How are the regions created in a decision tree?", "answers": ["The regions are created by recursive binary splitting of the predictor space into high-dimensional rectangles or boxes."]},
{"id": "33", "contributed_by": "group 3", "question": "What is recursive binary splitting?", "answers": ["Recursive binary splitting is a greedy top-down approach that involves splitting the predictor space into two regions, and then repeating the process on each of the resulting regions to further split the space."]},
{"id": "34", "contributed_by": "group 3", "question": "What is used as the criterion to find the best split at each step when building a regression tree?", "answers": ["The best split is the one that leads to the greatest reduction in RSS (residual sum of squares)."]},
{"id": "35", "contributed_by": "group 3", "question": "How are classification trees different from regression trees?", "answers": ["Classification trees predict a qualitative response by using the most commonly occurring class in each region, rather than the mean response value used in regression trees."]},
{"id": "36", "contributed_by": "group 3", "question": "What are some advantages of decision trees over linear models?", "answers": ["Advantages include ease of interpretation, ability to capture complex relationships, and built-in feature selection."]},
{"id": "37", "contributed_by": "group 3", "question": "What are terminal nodes in a decision tree called?", "answers": ["Terminal nodes are also known as leaves."]},
{"id": "38", "contributed_by": "group 3", "question": "What is tree pruning and why is it useful?", "answers": ["Tree pruning involves cutting back a fully grown tree to obtain a subtree in order to avoid overfitting. It trades a little bias for a reduction in variance."]},
{"id": "39", "contributed_by": "group 3", "question": "How does cost complexity pruning work?", "answers": ["Cost complexity pruning generates a sequence of subtrees indexed by a tuning parameter that controls the tradeoff between tree size and fit to the training data."]},
{"id": "40", "contributed_by": "group 3", "question": "What is bagging?", "answers": ["Bagging involves creating multiple copies of the original training data set using bootstrap sampling, fitting a decision tree to each, and then combining the predictions."]},
{"id": "41", "contributed_by": "group 3", "question": "How does random forests improve upon bagging?", "answers": ["Random forests reduce correlation between trees by splitting each node using a subset of features chosen at random."]},
{"id": "42", "contributed_by": "group 3", "question": "What is boosting?", "answers": ["Boosting involves fitting a sequence of trees on modified versions of the data, where each successive tree is fit on the residuals from the previous tree."]},
{"id": "43", "contributed_by": "group 3", "question": "How does boosting learn slowly?", "answers": ["Each new tree in boosting is shrunk and fits the residual from the current ensemble. The learning rate hyperparameter further slows learning."]},
{"id": "44", "contributed_by": "group 3", "question": "What is Bayesian Additive Regression Trees (BART)?", "answers": ["BART is an ensemble method that fits successive trees by perturbing the previous tree to avoid overfitting. It can be seen as a Bayesian approach."]},
{"id": "45", "contributed_by": "group 3", "question": "What is the out-of-bag error in bagging?", "answers": ["The out-of-bag error provides an estimate of the test error by using predictions from those trees that did not use a given observation."]},
{"id": "46", "contributed_by": "group 3", "question": "How are qualitative predictors handled in decision trees?", "answers": ["Decision trees can split qualitative predictors without needing dummy variable encoding, by partitioning their values."]},
{"id": "47", "contributed_by": "group 3", "question": "What are the components of a decision tree?", "answers": ["The components are internal nodes, terminal nodes (leaves), branches, and regions."]},
{"id": "48", "contributed_by": "group 3", "question": "What is the deviance?", "answers": ["The deviance indicates how well the tree fits the training data. Lower deviance is better."]},
{"id": "49", "contributed_by": "group 3", "question": "What is the Gini index?", "answers": ["The Gini index is a measure of total variance across classes that is used to evaluate splits when building classification trees."]},
{"id": "50", "contributed_by": "group 3", "question": "What is entropy?", "answers": ["Entropy is a measure of node purity used as a criterion for choosing splits in classification trees."]},
{"id": "51", "contributed_by": "group 3", "question": "How are majority votes used in bagging classification trees?", "answers": ["Each tree makes a class prediction for a given observation. The overall prediction is the most common class prediction across all trees."]},
{"id": "52", "contributed_by": "group 3", "question": "What are weak learners?", "answers": ["Weak learners are simple models like single decision trees that can be combined into a more powerful ensemble."]},
{"id": "53", "contributed_by": "group 3", "question": "What is the interaction depth in boosting?", "answers": ["The interaction depth controls the complexity of boosted trees. Depth 1 corresponds to an additive model."]},
{"id": "54", "contributed_by": "group 3", "question": "What are variable importance measures?", "answers": ["Variable importance measures summarize the predictive value of each feature, such as mean Gini reduction."]},
{"id": "55", "contributed_by": "group 3", "question": "How does BART work?", "answers": ["BART fits successive trees by randomly perturbing the previous tree to avoid overfitting."]},
{"id": "56", "contributed_by": "group 3", "question": "What are the tuning parameters in boosting?", "answers": ["Important tuning parameters in boosting include the number of trees B, the learning rate, and the interaction depth."]},
{"id": "57", "contributed_by": "group 3", "question": "What is the burn-in period in BART?", "answers": ["The burn-in period refers to the initial iterations in BART that are discarded before computing the average prediction."]},
{"id": "58", "contributed_by": "group 3", "question": "How does random forests improve over bagging?", "answers": ["Random forests reduce correlation between trees by splitting nodes using random subsets of features."]},
{"id": "59", "contributed_by": "group 3", "question": "What is the difference between regression trees and linear models?", "answers": ["Regression trees partition the feature space into regions with constant prediction within each region, while linear models assume a global linear relationship."]}