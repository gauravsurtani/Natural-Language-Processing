[
  {
    "id": "30",
    "title": "",
    "section": "",
    "text": "In this chapter, we describe tree-based methods for regression and classification. These involve stratifying or segmenting the predictor space into a number of simple regions. Decision trees can be applied to both regression and classification problems. We first consider regression problems, and then move on to classification."
  },
  {
    "id": "31",
    "title": "",
    "section": "",
    "text": "We now elaborate on Step 1 above. How do we construct the regions R1, . . . , RJ? In order to make a prediction for a given observation, we typically use the mean or the mode response value for the training observations in the region to which it belongs. For every observation that falls into the region Rj, we make the same prediction, which is simply the mean of the response values for the training observations in Rj."
  },
  {
    "id": "32",
    "title": "",
    "section": "",
    "text": "In theory, the regions could have any shape. However, we choose to divide the predictor space into high-dimensional rectangles, or boxes, for simplicity and for ease of interpretation of the resulting predictive model. The goal is to find boxes R1, . . . , RJ that minimize the RSS, given by ∑J j=1 ∑i∈Rj (yi − ˆyRj )2, where ˆyRj is the mean response for the training observations within the jth box."
  },
  {
    "id": "33",
    "title": "",
    "section": "",
    "text": "Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into J boxes. For this reason, we take a top-down, greedy approach that is known as recursive binary splitting. The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree."
  },
  {
    "id": "34",
    "title": "",
    "section": "",
    "text": "Next, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions. However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions. We now have three regions. Again, we look to split one of these three regions further, so as to minimize the RSS. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations."
  },
  {
    "id": "35",
    "title": "",
    "section": "",
    "text": "A classification tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one. Recall that for a regression tree, the predicted response for an observation is given by the mean response of the training observations that belong to the same terminal node. In contrast, for a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs."
  },
  {
    "id": "36",
    "title": "",
    "section": "",
    "text": "Decision trees for regression and classification have a number of advantages over the more classical approaches seen in Chapters 3 and 4: Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression! Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters!"
  },
  {
    "id": "37",
    "title": "",
    "section": "",
    "text": "Once the regions R1, . . . , RJ have been created, we predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs. A five-region example of this approach is shown in Figure 8.3. In keeping with the tree analogy, the regions R1, R2, . . . , RJ are known as terminal nodes or leaves of the tree."
  },
  {
    "id": "38",
    "title": "",
    "section": "",
    "text": "The process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance. This is because the resulting tree might be too complex. A smaller tree with fewer splits (that is, fewer regions R1, . . . , RJ) might lead to lower variance and better interpretation at the cost of a little bias."
  },
  {
    "id": "39",
    "title": "",
    "section": "",
    "text": "Cost complexity pruning—also known as weakest link pruning—gives us a way to do just this. Rather than considering every possible subtree, we consider a sequence of trees indexed by a nonnegative tuning parameter α. For each value of α there corresponds a subtree T ⊂ T0 such that |T |∑m=1 ∑i:xi∈Rm (yi − ˆyRm )2 + α|T | is as small as possible."
  },
  {
    "id": "40",
    "title": "",
    "section": "",
    "text": "Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method; we introduce it here because it is particularly useful and frequently used in the context of decision trees. Recall that given a set of n independent observations Z1, . . . , Zn, each with variance σ2, the variance of the mean Z ̄ of the observations is given by σ2/n."
  },
  {
    "id": "41",
    "title": "",
    "section": "",
    "text": "Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors."
  },
  {
    "id": "42",
    "title": "",
    "section": "",
    "text": "Boosting works in a similar way, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Consider first the regression setting. Like bagging, boosting involves combining a large number of decision trees, ˆf1, . . . , ˆfB."
  },
  {
    "id": "43",
    "title": "",
    "section": "",
    "text": "Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals"
  },
  {
    "id": "44",
    "title": "",
    "section": "",
    "text": "Finally, we discuss Bayesian additive regression trees (BART), another ensemble method that uses decision trees as its building blocks. For simplicity, we present BART for regression (as opposed to classification). Recall that bagging and random forests make predictions from an average of regression trees, each of which is built using a random sample of data and/or predictors."
  },
  {
    "id": "45",
    "title": "",
    "section": "",
    "text": "It turns out that there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations."
  },
  {
    "id": "46",
    "title": "",
    "section": "",
    "text": "Thus far, we have assumed that the predictor variables take on continuous values. However, decision trees can be constructed even in the presence of qualitative predictor variables. For instance, in the Heart data, some of the predictors, such as Sex, Thal (Thallium stress test), and ChestPain, are qualitative."
  },
  {
    "id": "47",
    "title": "",
    "section": "",
    "text": "Once the regions R1, . . . , RJ have been created, we predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs. A five-region example of this approach is shown in Figure 8.3. In keeping with the tree analogy, the regions R1, R2, . . . , RJ are known as terminal nodes or leaves of the tree."
  },
  {
    "id": "48",
    "title": "",
    "section": "",
    "text": "We can access the value of the deviance using log_loss(), −2∑m ∑k nmk log ˆpmk, where nmk is the number of observations in the mth terminal node that belong to the kth class. A small value indicates a tree that provides a good fit to the (training) data."
  },
  {
    "id": "49",
    "title": "",
    "section": "",
    "text": "The Gini index is defined by G = ∑K k=1 ˆpmk(1 − ˆpmk), a measure of total variance across the K classes. It is not hard to see that the Gini index takes on a small value if all of the ˆpmk’s are close to zero or one. For this reason the Gini index is referred to as a measure of node purity—a small value indicates that a node contains predominantly observations from a single class."
  },
  {
    "id": "50",
    "title": "",
    "section": "",
    "text": "An alternative to the Gini index is entropy, given by D = −∑K k=1 ˆpmk log ˆpmk. Since 0 ≤ ˆpmk ≤ 1, it follows that 0 ≤ −ˆpmk log ˆpmk. One can show that the entropy will take on a value near zero if the ˆpmk’s are all near zero or near one. Therefore, like the Gini index, the entropy will take on a small value if the mth node is pure."
  },
  {
    "id": "51",
    "title": "",
    "section": "",
    "text": "Boosting has been demonstrated to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure. Thus far, we have described the bagging procedure in the regression context, to predict a quantitative outcome Y. How can bagging be extended to a classification problem where Y is qualitative?"
  },
  {
    "id": "52",
    "title": "",
    "section": "",
    "text": "An ensemble method is an approach that combines many simple “building block” models in order to obtain a single and potentially very powerful model. These simple building block models are sometimes known as weak learners, since they may lead to mediocre predictions on their own."
  },
  {
    "id": "53",
    "title": "",
    "section": "",
    "text": "More generally d is the interaction depth, and controls the interaction order of the boosted model, since d splits can involve at most d variables. In general, statistical learning approaches that learn slowly tend to perform well. Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown."
  },
  {
    "id": "54",
    "title": "",
    "section": "",
    "text": "In the case of bagging regression trees, we can record the total amount that the RSS (8.1) is decreased due to splits over a given predictor, averaged over all B trees. A large value indicates an important predictor. Similarly, in the context of bagging classification trees, we can add up the total amount that the Gini index (8.6) is decreased by splits over a given predictor, averaged over all B trees."
  },
  {
    "id": "55",
    "title": "",
    "section": "",
    "text": "BART is related to both approaches: each tree is constructed in a random manner as in bagging and random forests, and each tree tries to capture signal not yet accounted for by the current model, as in boosting. The main novelty in BART is the way in which new trees are generated."
  },
  {
    "id": "56",
    "title": "",
    "section": "",
    "text": "We have just described the process of boosting regression trees. Boosting classification trees proceeds in a similar but slightly more complex way, and the details are omitted here. Boosting has three tuning parameters: 1. The number of trees B. Unlike bagging and random forests, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select B."
  },

  {
    "id": "57",
    "title": "",
    "section": "",
    "text": "We typically throw away the first few of these prediction models, since models obtained in the earlier iterations — known as the burn-in period — tend not to provide very good results. However, it is also possible to compute quantities other than the average: for instance, the percentiles of fL+1(x), . . . , fB(x) provide a measure of uncertainty in the final prediction."
  },
  {
    "id": "58",
    "title": "",
    "section": "",
    "text": "Random forests overcome this problem by forcing each split to consider only a subset of the predictors. Therefore, on average (p − m)/p of the splits will not even consider the strong predictor, and so other predictors will have more of a chance. We can think of this process as decorrelating the trees, thereby making the average of the resulting trees less variable and hence more reliable."
  },
  {
    "id": "59",
    "title": "",
    "section": "",
    "text": "Regression and classification trees have a very different flavor from the more classical approaches for regression and classification presented in Chapters 3 and 4. In particular, linear regression assumes a model of the form f(X) = β0 + ∑p j=1 Xjβj, whereas regression trees assume a model of the form f(X) = ∑M m=1 cm · 1(X∈Rm) where R1, . . . , RM represent a partition of feature space, as in Figure 8.3."
  }
]
