[
  {
    "id": "30",
    "contributed_by": "group 3",
    "question": "What are tree-based methods used for?",
    "answers": [
      "Tree-based methods are used for regression and classification."
    ],
    "referenced_from": "In this chapter, we describe tree-based methods for regression and classification. These involve stratifying or segmenting the predictor space into a number of simple regions. Decision trees can be applied to both regression and classification problems. We first consider regression problems, and then move on to classification."
  },

  {
    "id": "31",
    "contributed_by": "group 3",
    "question": "What are the two main steps for predicting using a regression tree?",
    "answers": [
      "1. Divide the predictor space into distinct non-overlapping regions. 2. For each region, make a prediction using the mean response of training observations in that region."
    ],
    "referenced_from": "We now elaborate on Step 1 above. How do we construct the regions R1, . . . , RJ? In order to make a prediction for a given observation, we typically use the mean or the mode response value for the training observations in the region to which it belongs. For every observation that falls into the region Rj, we make the same prediction, which is simply the mean of the response values for the training observations in Rj."
  },

  {
    "id": "32",
    "contributed_by": "group 3",
    "question": "How are the regions created in a decision tree?",
    "answers": [
      "The regions are created by recursive binary splitting of the predictor space into high-dimensional rectangles or boxes."
    ],
    "referenced_from": "In theory, the regions could have any shape. However, we choose to divide the predictor space into high-dimensional rectangles, or boxes, for simplicity and for ease of interpretation of the resulting predictive model. The goal is to find boxes R1, . . . , RJ that minimize the RSS, given by ∑J j=1 ∑i∈Rj (yi − ˆyRj )2, where ˆyRj is the mean response for the training observations within the jth box."
  },

  {
    "id": "33",
    "contributed_by": "group 3",
    "question": "What is recursive binary splitting?",
    "answers": [
      "Recursive binary splitting is a greedy top-down approach that involves splitting the predictor space into two regions, and then repeating the process on each of the resulting regions to further split the space."
    ],
    "referenced_from": "Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into J boxes. For this reason, we take a top-down, greedy approach that is known as recursive binary splitting. The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree."
  },

  {
    "id": "34",
    "contributed_by": "group 3",
    "question": "What is used as the criterion to find the best split at each step when building a regression tree?",
    "answers": [
      "The best split is the one that leads to the greatest reduction in RSS (residual sum of squares)."
    ],
    "referenced_from": "Next, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions. However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions. We now have three regions. Again, we look to split one of these three regions further, so as to minimize the RSS. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations."
  },

  {
    "id": "35",
    "contributed_by": "group 3",
    "question": "How are classification trees different from regression trees?",
    "answers": [
      "Classification trees predict a qualitative response by using the most commonly occurring class in each region, rather than the mean response value used in regression trees."
    ],
    "referenced_from": "A classification tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one. Recall that for a regression tree, the predicted response for an observation is given by the mean response of the training observations that belong to the same terminal node. In contrast, for a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs."
  },

  {
    "id": "36",
    "contributed_by": "group 3",
    "question": "What are some advantages of decision trees over linear models?",
    "answers": [
      "Advantages include ease of interpretation, ability to capture complex relationships, and built-in feature selection."
    ],
    "referenced_from": "Decision trees for regression and classification have a number of advantages over the more classical approaches seen in Chapters 3 and 4: Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression! Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters!"
  },

  {
    "id": "37",
    "contributed_by": "group 3",
    "question": "What are terminal nodes in a decision tree called?",
    "answers": ["Terminal nodes are also known as leaves."],
    "referenced_from": "Once the regions R1, . . . , RJ have been created, we predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs. A five-region example of this approach is shown in Figure 8.3. In keeping with the tree analogy, the regions R1, R2, . . . , RJ are known as terminal nodes or leaves of the tree."
  },

  {
    "id": "38",
    "contributed_by": "group 3",
    "question": "What is tree pruning and why is it useful?",
    "answers": [
      "Tree pruning involves cutting back a fully grown tree to obtain a subtree in order to avoid overfitting. It trades a little bias for a reduction in variance."
    ],
    "referenced_from": "The process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance. This is because the resulting tree might be too complex. A smaller tree with fewer splits (that is, fewer regions R1, . . . , RJ) might lead to lower variance and better interpretation at the cost of a little bias."
  },

  {
    "id": "39",
    "contributed_by": "group 3",
    "question": "How does cost complexity pruning work?",
    "answers": [
      "Cost complexity pruning generates a sequence of subtrees indexed by a tuning parameter that controls the tradeoff between tree size and fit to the training data."
    ],
    "referenced_from": "Cost complexity pruning—also known as weakest link pruning—gives us a way to do just this. Rather than considering every possible subtree, we consider a sequence of trees indexed by a nonnegative tuning parameter α. For each value of α there corresponds a subtree T ⊂ T0 such that |T |∑m=1 ∑i:xi∈Rm (yi − ˆyRm )2 + α|T | is as small as possible."
  },

  {
    "id": "40",
    "contributed_by": "group 3",
    "question": "What is bagging?",
    "answers": [
      "Bagging involves creating multiple copies of the original training data set using bootstrap sampling, fitting a decision tree to each, and then combining the predictions."
    ],
    "referenced_from": "Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method; we introduce it here because it is particularly useful and frequently used in the context of decision trees. Recall that given a set of n independent observations Z1, . . . , Zn, each with variance σ2, the variance of the mean Z ̄ of the observations is given by σ2/n."
  },

  {
    "id": "41",
    "contributed_by": "group 3",
    "question": "How does random forests improve upon bagging?",
    "answers": [
      "Random forests reduce correlation between trees by splitting each node using a subset of features chosen at random."
    ],
    "referenced_from": "Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors."
  },

  {
    "id": "42",
    "contributed_by": "group 3",
    "question": "What is boosting?",
    "answers": [
      "Boosting involves fitting a sequence of trees on modified versions of the data, where each successive tree is fit on the residuals from the previous tree."
    ],
    "referenced_from": "Boosting works in a similar way, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Consider first the regression setting. Like bagging, boosting involves combining a large number of decision trees, ˆf1, . . . , ˆfB."
  },

  {
    "id": "43",
    "contributed_by": "group 3",
    "question": "How does boosting learn slowly?",
    "answers": [
      "Each new tree in boosting is shrunk and fits the residual from the current ensemble. The learning rate hyperparameter further slows learning."
    ],
    "referenced_from": "Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome Y , as the response. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter d in the algorithm. By fitting small trees to the residuals, we slowly improve fˆ in areas where it does not perform well."
  },

  {
    "id": "44",
    "contributed_by": "group 3",
    "question": "What is Bayesian Additive Regression Trees (BART)?",
    "answers": [
      "BART is an ensemble method that fits successive trees by perturbing the previous tree to avoid overfitting. It can be seen as a Bayesian approach."
    ],
    "referenced_from": "Finally, we discuss Bayesian additive regression trees (BART), another ensemble method that uses decision trees as its building blocks. For simplicity, we present BART for regression (as opposed to classification). Recall that bagging and random forests make predictions from an average of regression trees, each of which is built using a random sample of data and/or predictors."
  },

  {
    "id": "45",
    "contributed_by": "group 3",
    "question": "What is the out-of-bag error in bagging?",
    "answers": [
      "The out-of-bag error provides an estimate of the test error by using predictions from those trees that did not use a given observation."
    ],
    "referenced_from": "It turns out that there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations."
  },

  {
    "id": "46",
    "contributed_by": "group 3",
    "question": "How are qualitative predictors handled in decision trees?",
    "answers": [
      "Decision trees can split qualitative predictors without needing dummy variable encoding, by partitioning their values."
    ],
    "referenced_from": "Thus far, we have assumed that the predictor variables take on continuous values. However, decision trees can be constructed even in the presence of qualitative predictor variables. For instance, in the Heart data, some of the predictors, such as Sex, Thal (Thallium stress test), and ChestPain, are qualitative."
  },

  {
    "id": "47",
    "contributed_by": "group 3",
    "question": "What are the components of a decision tree?",
    "answers": [
      "The components are internal nodes, terminal nodes (leaves), branches, and regions."
    ],
    "referenced_from": "Once the regions R1, . . . , RJ have been created, we predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs. A five-region example of this approach is shown in Figure 8.3. In keeping with the tree analogy, the regions R1, R2, . . . , RJ are known as terminal nodes or leaves of the tree."
  },

  {
    "id": "48",
    "contributed_by": "group 3",
    "question": "What is the deviance?",
    "answers": [
      "The deviance indicates how well the tree fits the training data. Lower deviance is better."
    ],
    "referenced_from": "We can access the value of the deviance using log_loss(), −2∑m ∑k nmk log ˆpmk, where nmk is the number of observations in the mth terminal node that belong to the kth class. A small value indicates a tree that provides a good fit to the (training) data."
  },

  {
    "id": "49",
    "contributed_by": "group 3",
    "question": "What is the Gini index?",
    "answers": [
      "The Gini index is a measure of total variance across classes that is used to evaluate splits when building classification trees."
    ],
    "referenced_from": "The Gini index is defined by G = ∑K k=1 ˆpmk(1 − ˆpmk), a measure of total variance across the K classes. It is not hard to see that the Gini index takes on a small value if all of the ˆpmk’s are close to zero or one. For this reason the Gini index is referred to as a measure of node purity—a small value indicates that a node contains predominantly observations from a single class."
  },

  {
    "id": "50",
    "contributed_by": "group 3",
    "question": "What is entropy?",
    "answers": [
      "Entropy is a measure of node purity used as a criterion for choosing splits in classification trees."
    ],
    "referenced_from": "An alternative to the Gini index is entropy, given by D = −∑K k=1 ˆpmk log ˆpmk. Since 0 ≤ ˆpmk ≤ 1, it follows that 0 ≤ −ˆpmk log ˆpmk. One can show that the entropy will take on a value near zero if the ˆpmk’s are all near zero or near one. Therefore, like the Gini index, the entropy will take on a small value if the mth node is pure."
  },

  {
    "id": "51",
    "contributed_by": "group 3",
    "question": "How are majority votes used in bagging classification trees?",
    "answers": [
      "Each tree makes a class prediction for a given observation. The overall prediction is the most common class prediction across all trees."
    ],
    "referenced_from": "Boosting has been demonstrated to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure. Thus far, we have described the bagging procedure in the regression context, to predict a quantitative outcome Y. How can bagging be extended to a classification problem where Y is qualitative?"
  },

  {
    "id": "52",
    "contributed_by": "group 3",
    "question": "What are weak learners?",
    "answers": [
      "Weak learners are simple models like single decision trees that can be combined into a more powerful ensemble."
    ],
    "referenced_from": "An ensemble method is an approach that combines many simple “building block” models in order to obtain a single and potentially very powerful model. These simple building block models are sometimes known as weak learners, since they may lead to mediocre predictions on their own."
  },

  {
    "id": "53",
    "contributed_by": "group 3",
    "question": "What is the interaction depth in boosting?",
    "answers": [
      "The interaction depth controls the complexity of boosted trees. Depth 1 corresponds to an additive model."
    ],
    "referenced_from": "More generally d is the interaction depth, and controls the interaction order of the boosted model, since d splits can involve at most d variables. In general, statistical learning approaches that learn slowly tend to perform well. Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown."
  },

  {
    "id": "54",
    "contributed_by": "group 3",
    "question": "What are variable importance measures?",
    "answers": [
      "Variable importance measures summarize the predictive value of each feature, such as mean Gini reduction."
    ],
    "referenced_from": "In the case of bagging regression trees, we can record the total amount that the RSS (8.1) is decreased due to splits over a given predictor, averaged over all B trees. A large value indicates an important predictor. Similarly, in the context of bagging classification trees, we can add up the total amount that the Gini index (8.6) is decreased by splits over a given predictor, averaged over all B trees."
  },

  {
    "id": "55",
    "contributed_by": "group 3",
    "question": "How does BART work?",
    "answers": [
      "BART fits successive trees by randomly perturbing the previous tree to avoid overfitting."
    ],
    "referenced_from": "BART is related to both approaches: each tree is constructed in a random manner as in bagging and random forests, and each tree tries to capture signal not yet accounted for by the current model, as in boosting. The main novelty in BART is the way in which new trees are generated."
  },

  {
    "id": "56",
    "contributed_by": "group 3",
    "question": "What are the tuning parameters in boosting?",
    "answers": [
      "Important tuning parameters in boosting include the number of trees B, the learning rate, and the interaction depth."
    ],
    "referenced_from": "We have just described the process of boosting regression trees. Boosting classification trees proceeds in a similar but slightly more complex way, and the details are omitted here. Boosting has three tuning parameters: 1. The number of trees B. Unlike bagging and random forests, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select B. 2"
  },
  {
    "id": "57",
    "contributed_by": "group 3",
    "question": "What is the burn-in period in BART?",
    "answers": [
      "The burn-in period refers to the initial iterations in BART that are discarded before computing the average prediction."
    ],
    "referenced_from": "We typically throw away the first few of these prediction models, since models obtained in the earlier iterations — known as the burn-in period — tend not to provide very good results. However, it is also possible to compute quantities other than the average: for instance, the percentiles of fL+1(x), . . . , fB(x) provide a measure of uncertainty in the final prediction."
  },
  {
    "id": "58",
    "contributed_by": "group 3",
    "question": "How does random forests improve over bagging?",
    "answers": [
      "Random forests reduce correlation between trees by splitting nodes using random subsets of features."
    ],
    "referenced_from": "Random forests overcome this problem by forcing each split to consider only a subset of the predictors. Therefore, on average (p − m)/p of the splits will not even consider the strong predictor, and so other predictors will have more of a chance. We can think of this process as decorrelating the trees, thereby making the average of the resulting trees less variable and hence more reliable."
  },
  {
    "id": "59",
    "contributed_by": "group 3",
    "question": "What is the difference between regression trees and linear models?",
    "answers": [
      "Regression trees partition the feature space into regions with constant prediction within each region, while linear models assume a global linear relationship."
    ],
    "referenced_from": "Regression and classification trees have a very different flavor from the more classical approaches for regression and classification presented in Chapters 3 and 4. In particular, linear regression assumes a model of the form f(X) = β0 + ∑p j=1 Xjβj, whereas regression trees assume a model of the form f(X) = ∑M m=1 cm · 1(X∈Rm) where R1, . . . , RM represent a partition of feature space, as in Figure 8.3."
  }
]
