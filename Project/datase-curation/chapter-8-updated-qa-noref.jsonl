{
"id": "30",
"contributed_by": "group 3",
"question": "What are tree-based methods used for?",
"answers": [
    "Tree-based methods are used for regression and classification."
],
},

{
"id": "31",
"contributed_by": "group 3",
"question": "What are the two main steps for predicting using a regression tree?",
"answers": [
    "1. Divide the predictor space into distinct non-overlapping regions. 2. For each region, make a prediction using the mean response of training observations in that region."
],
},

{
"id": "32",
"contributed_by": "group 3",
"question": "How are the regions created in a decision tree?",
"answers": [
    "The regions are created by recursive binary splitting of the predictor space into high-dimensional rectangles or boxes."
],
},

{
"id": "33",
"contributed_by": "group 3",
"question": "What is recursive binary splitting?",
"answers": [
    "Recursive binary splitting is a greedy top-down approach that involves splitting the predictor space into two regions, and then repeating the process on each of the resulting regions to further split the space."
],
},

{
"id": "34",
"contributed_by": "group 3",
"question": "What is used as the criterion to find the best split at each step when building a regression tree?",
"answers": [
    "The best split is the one that leads to the greatest reduction in RSS (residual sum of squares)."
],
},

{
"id": "35",
"contributed_by": "group 3",
"question": "How are classification trees different from regression trees?",
"answers": [
    "Classification trees predict a qualitative response by using the most commonly occurring class in each region, rather than the mean response value used in regression trees."
],
},

{
"id": "36",
"contributed_by": "group 3",
"question": "What are some advantages of decision trees over linear models?",
"answers": [
    "Advantages include ease of interpretation, ability to capture complex relationships, and built-in feature selection."
],
},

{
"id": "37",
"contributed_by": "group 3",
"question": "What are terminal nodes in a decision tree called?",
"answers": ["Terminal nodes are also known as leaves."],
},

{
"id": "38",
"contributed_by": "group 3",
"question": "What is tree pruning and why is it useful?",
"answers": [
    "Tree pruning involves cutting back a fully grown tree to obtain a subtree in order to avoid overfitting. It trades a little bias for a reduction in variance."
],
},

{
"id": "39",
"contributed_by": "group 3",
"question": "How does cost complexity pruning work?",
"answers": [
    "Cost complexity pruning generates a sequence of subtrees indexed by a tuning parameter that controls the tradeoff between tree size and fit to the training data."
],
},

{
"id": "40",
"contributed_by": "group 3",
"question": "What is bagging?",
"answers": [
    "Bagging involves creating multiple copies of the original training data set using bootstrap sampling, fitting a decision tree to each, and then combining the predictions."
],
},

{
"id": "41",
"contributed_by": "group 3",
"question": "How does random forests improve upon bagging?",
"answers": [
    "Random forests reduce correlation between trees by splitting each node using a subset of features chosen at random."
],
},

{
"id": "42",
"contributed_by": "group 3",
"question": "What is boosting?",
"answers": [
    "Boosting involves fitting a sequence of trees on modified versions of the data, where each successive tree is fit on the residuals from the previous tree."
],
},

{
"id": "43",
"contributed_by": "group 3",
"question": "How does boosting learn slowly?",
"answers": [
    "Each new tree in boosting is shrunk and fits the residual from the current ensemble. The learning rate hyperparameter further slows learning."
],
},

{
"id": "44",
"contributed_by": "group 3",
"question": "What is Bayesian Additive Regression Trees (BART)?",
"answers": [
    "BART is an ensemble method that fits successive trees by perturbing the previous tree to avoid overfitting. It can be seen as a Bayesian approach."
],
},

{
"id": "45",
"contributed_by": "group 3",
"question": "What is the out-of-bag error in bagging?",
"answers": [
    "The out-of-bag error provides an estimate of the test error by using predictions from those trees that did not use a given observation."
],
},

{
"id": "46",
"contributed_by": "group 3",
"question": "How are qualitative predictors handled in decision trees?",
"answers": [
    "Decision trees can split qualitative predictors without needing dummy variable encoding, by partitioning their values."
],
},

{
"id": "47",
"contributed_by": "group 3",
"question": "What are the components of a decision tree?",
"answers": [
    "The components are internal nodes, terminal nodes (leaves), branches, and regions."
],
},

{
"id": "48",
"contributed_by": "group 3",
"question": "What is the deviance?",
"answers": [
    "The deviance indicates how well the tree fits the training data. Lower deviance is better."
],
},

{
"id": "49",
"contributed_by": "group 3",
"question": "What is the Gini index?",
"answers": [
    "The Gini index is a measure of total variance across classes that is used to evaluate splits when building classification trees."
],
},

{
"id": "50",
"contributed_by": "group 3",
"question": "What is entropy?",
"answers": [
    "Entropy is a measure of node purity used as a criterion for choosing splits in classification trees."
],
},

{
"id": "51",
"contributed_by": "group 3",
"question": "How are majority votes used in bagging classification trees?",
"answers": [
    "Each tree makes a class prediction for a given observation. The overall prediction is the most common class prediction across all trees."
],
},

{
"id": "52",
"contributed_by": "group 3",
"question": "What are weak learners?",
"answers": [
    "Weak learners are simple models like single decision trees that can be combined into a more powerful ensemble."
],
},

{
"id": "53",
"contributed_by": "group 3",
"question": "What is the interaction depth in boosting?",
"answers": [
    "The interaction depth controls the complexity of boosted trees. Depth 1 corresponds to an additive model."
],
},

{
"id": "54",
"contributed_by": "group 3",
"question": "What are variable importance measures?",
"answers": [
    "Variable importance measures summarize the predictive value of each feature, such as mean Gini reduction."
],
},

{
"id": "55",
"contributed_by": "group 3",
"question": "How does BART work?",
"answers": [
    "BART fits successive trees by randomly perturbing the previous tree to avoid overfitting."
],
},

{
"id": "56",
"contributed_by": "group 3",
"question": "What are the tuning parameters in boosting?",
"answers": [
    "Important tuning parameters in boosting include the number of trees B, the learning rate, and the interaction depth."
],
},
{
"id": "57",
"contributed_by": "group 3",
"question": "What is the burn-in period in BART?",
"answers": [
    "The burn-in period refers to the initial iterations in BART that are discarded before computing the average prediction."
],
},
{
"id": "58",
"contributed_by": "group 3",
"question": "How does random forests improve over bagging?",
"answers": [
    "Random forests reduce correlation between trees by splitting nodes using random subsets of features."
],
},
{
"id": "59",
"contributed_by": "group 3",
"question": "What is the difference between regression trees and linear models?",
"answers": [
    "Regression trees partition the feature space into regions with constant prediction within each region, while linear models assume a global linear relationship."
],
}